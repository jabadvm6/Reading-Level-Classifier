{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Level Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/javm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileName</th>\n",
       "      <th>AoA_Bird_Lem</th>\n",
       "      <th>AoA_Bristol_Lem</th>\n",
       "      <th>AoA_Cort_Lem</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>AoA_Kup_Lem</th>\n",
       "      <th>DISC_RefExprDefArtPerSen</th>\n",
       "      <th>DISC_RefExprDefArtPerWord</th>\n",
       "      <th>DISC_RefExprPerProPerWord</th>\n",
       "      <th>DISC_RefExprPerPronounsPerSen</th>\n",
       "      <th>...</th>\n",
       "      <th>PROPORTION_INDEF_NP_REFCHAIN</th>\n",
       "      <th>PROPORTION_DEF_NP_REFCHAIN</th>\n",
       "      <th>PROPORTION_PERSONAL_PRONOUNS_REFCHAIN</th>\n",
       "      <th>PROPORTION_POSSESIVE_DETERMINERS_REFCHAIN</th>\n",
       "      <th>PROPORTION_DEMONSTRATIVE_DETERMINERS_REFCHAIN</th>\n",
       "      <th>PROPORTION_DEMONSTRATIVE_PRONOUNS_REFCHAIN</th>\n",
       "      <th>PROPORTION_REFLEXIVE_PRONOUNS_REFCHAIN</th>\n",
       "      <th>PROPORTION_PROPER_NOUNS_REFCHAIN</th>\n",
       "      <th>AVG_LEN_REFCHAIN</th>\n",
       "      <th>readinglevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon-adv.txt</td>\n",
       "      <td>3.36</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.44</td>\n",
       "      <td>4.67</td>\n",
       "      <td>6.10</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>adv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon-ele.txt</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2.43</td>\n",
       "      <td>4.56</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>ele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon-int.txt</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.43</td>\n",
       "      <td>4.60</td>\n",
       "      <td>5.93</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amsterdam-adv.txt</td>\n",
       "      <td>3.77</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.57</td>\n",
       "      <td>4.43</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>3.545455</td>\n",
       "      <td>adv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amsterdam-ele.txt</td>\n",
       "      <td>3.82</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.74</td>\n",
       "      <td>4.36</td>\n",
       "      <td>5.51</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>3.692308</td>\n",
       "      <td>ele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>Zero-Hours-ele.txt</td>\n",
       "      <td>3.98</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.55</td>\n",
       "      <td>4.47</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>3.727273</td>\n",
       "      <td>ele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>Zero-Hours-int.txt</td>\n",
       "      <td>4.06</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.52</td>\n",
       "      <td>4.30</td>\n",
       "      <td>5.98</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.61</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>3.615385</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>climate-change--adv.txt</td>\n",
       "      <td>3.81</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.45</td>\n",
       "      <td>4.33</td>\n",
       "      <td>5.91</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>adv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>climate-change--ele.txt</td>\n",
       "      <td>4.16</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.99</td>\n",
       "      <td>5.40</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>ele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>climate-change--int.txt</td>\n",
       "      <td>4.05</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.55</td>\n",
       "      <td>4.02</td>\n",
       "      <td>5.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>int</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567 rows × 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    fileName  AoA_Bird_Lem  AoA_Bristol_Lem  AoA_Cort_Lem  \\\n",
       "0             Amazon-adv.txt          3.36             1.50          2.44   \n",
       "1             Amazon-ele.txt          3.65             1.11          2.43   \n",
       "2             Amazon-int.txt          3.51             1.30          2.43   \n",
       "3          Amsterdam-adv.txt          3.77             1.35          2.57   \n",
       "4          Amsterdam-ele.txt          3.82             1.17          2.74   \n",
       "..                       ...           ...              ...           ...   \n",
       "562       Zero-Hours-ele.txt          3.98             1.48          2.55   \n",
       "563       Zero-Hours-int.txt          4.06             1.40          2.52   \n",
       "564  climate-change--adv.txt          3.81             1.41          2.45   \n",
       "565  climate-change--ele.txt          4.16             1.28          2.68   \n",
       "566  climate-change--int.txt          4.05             1.33          2.55   \n",
       "\n",
       "     AoA_Kup  AoA_Kup_Lem  DISC_RefExprDefArtPerSen  \\\n",
       "0       4.67         6.10                      1.25   \n",
       "1       4.56         5.76                      0.84   \n",
       "2       4.60         5.93                      1.12   \n",
       "3       4.43         5.94                      1.31   \n",
       "4       4.36         5.51                      1.13   \n",
       "..       ...          ...                       ...   \n",
       "562     4.47         5.98                      0.95   \n",
       "563     4.30         5.98                      1.46   \n",
       "564     4.33         5.91                      1.16   \n",
       "565     3.99         5.40                      0.96   \n",
       "566     4.02         5.72                      1.00   \n",
       "\n",
       "     DISC_RefExprDefArtPerWord  DISC_RefExprPerProPerWord  \\\n",
       "0                         0.07                       0.01   \n",
       "1                         0.06                       0.02   \n",
       "2                         0.07                       0.01   \n",
       "3                         0.05                       0.03   \n",
       "4                         0.05                       0.04   \n",
       "..                         ...                        ...   \n",
       "562                       0.04                       0.03   \n",
       "563                       0.05                       0.02   \n",
       "564                       0.05                       0.01   \n",
       "565                       0.05                       0.01   \n",
       "566                       0.05                       0.02   \n",
       "\n",
       "     DISC_RefExprPerPronounsPerSen  ...  PROPORTION_INDEF_NP_REFCHAIN  \\\n",
       "0                             0.22  ...                      0.200000   \n",
       "1                             0.27  ...                      0.115385   \n",
       "2                             0.25  ...                      0.166667   \n",
       "3                             0.89  ...                      0.179487   \n",
       "4                             0.90  ...                      0.208333   \n",
       "..                             ...  ...                           ...   \n",
       "562                           0.60  ...                      0.195122   \n",
       "563                           0.61  ...                      0.170213   \n",
       "564                           0.40  ...                      0.185185   \n",
       "565                           0.27  ...                      0.320000   \n",
       "566                           0.39  ...                      0.233333   \n",
       "\n",
       "     PROPORTION_DEF_NP_REFCHAIN  PROPORTION_PERSONAL_PRONOUNS_REFCHAIN  \\\n",
       "0                      0.485714                               0.200000   \n",
       "1                      0.615385                               0.230769   \n",
       "2                      0.466667                               0.233333   \n",
       "3                      0.461538                               0.358974   \n",
       "4                      0.375000                               0.416667   \n",
       "..                          ...                                    ...   \n",
       "562                    0.268293                               0.317073   \n",
       "563                    0.382979                               0.255319   \n",
       "564                    0.629630                               0.333333   \n",
       "565                    0.480000                               0.320000   \n",
       "566                    0.433333                               0.366667   \n",
       "\n",
       "     PROPORTION_POSSESIVE_DETERMINERS_REFCHAIN  \\\n",
       "0                                          0.0   \n",
       "1                                          0.0   \n",
       "2                                          0.0   \n",
       "3                                          0.0   \n",
       "4                                          0.0   \n",
       "..                                         ...   \n",
       "562                                        0.0   \n",
       "563                                        0.0   \n",
       "564                                        0.0   \n",
       "565                                        0.0   \n",
       "566                                        0.0   \n",
       "\n",
       "     PROPORTION_DEMONSTRATIVE_DETERMINERS_REFCHAIN  \\\n",
       "0                                              0.0   \n",
       "1                                              0.0   \n",
       "2                                              0.0   \n",
       "3                                              0.0   \n",
       "4                                              0.0   \n",
       "..                                             ...   \n",
       "562                                            0.0   \n",
       "563                                            0.0   \n",
       "564                                            0.0   \n",
       "565                                            0.0   \n",
       "566                                            0.0   \n",
       "\n",
       "     PROPORTION_DEMONSTRATIVE_PRONOUNS_REFCHAIN  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "..                                          ...   \n",
       "562                                         0.0   \n",
       "563                                         0.0   \n",
       "564                                         0.0   \n",
       "565                                         0.0   \n",
       "566                                         0.0   \n",
       "\n",
       "     PROPORTION_REFLEXIVE_PRONOUNS_REFCHAIN  PROPORTION_PROPER_NOUNS_REFCHAIN  \\\n",
       "0                                  0.000000                          0.171429   \n",
       "1                                  0.000000                          0.461538   \n",
       "2                                  0.000000                          0.233333   \n",
       "3                                  0.000000                          0.179487   \n",
       "4                                  0.000000                          0.166667   \n",
       "..                                      ...                               ...   \n",
       "562                                0.024390                          0.048780   \n",
       "563                                0.021277                          0.021277   \n",
       "564                                0.000000                          0.222222   \n",
       "565                                0.000000                          0.200000   \n",
       "566                                0.000000                          0.166667   \n",
       "\n",
       "     AVG_LEN_REFCHAIN  readinglevel  \n",
       "0            2.500000           adv  \n",
       "1            2.600000           ele  \n",
       "2            2.727273           int  \n",
       "3            3.545455           adv  \n",
       "4            3.692308           ele  \n",
       "..                ...           ...  \n",
       "562          3.727273           ele  \n",
       "563          3.615385           int  \n",
       "564          2.700000           adv  \n",
       "565          2.777778           ele  \n",
       "566          2.727273           int  \n",
       "\n",
       "[567 rows x 157 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_feat_df = pd.read_csv('/Users/javm/Desktop/Flatiron4/allfeatures-ose-final.csv')\n",
    "new_feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_df.rename(columns = {'readinglevel':'Level'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_df.rename(columns = {'fileName':'Title'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_df['Level'].replace(['adv', 'int', 'ele'],\n",
    "                        [2,1,0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>AoA_Bird_Lem</th>\n",
       "      <th>AoA_Bristol_Lem</th>\n",
       "      <th>AoA_Cort_Lem</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>AoA_Kup_Lem</th>\n",
       "      <th>DISC_RefExprDefArtPerSen</th>\n",
       "      <th>DISC_RefExprDefArtPerWord</th>\n",
       "      <th>DISC_RefExprPerProPerWord</th>\n",
       "      <th>DISC_RefExprPerPronounsPerSen</th>\n",
       "      <th>...</th>\n",
       "      <th>PROPORTION_INDEF_NP_REFCHAIN</th>\n",
       "      <th>PROPORTION_DEF_NP_REFCHAIN</th>\n",
       "      <th>PROPORTION_PERSONAL_PRONOUNS_REFCHAIN</th>\n",
       "      <th>PROPORTION_POSSESIVE_DETERMINERS_REFCHAIN</th>\n",
       "      <th>PROPORTION_DEMONSTRATIVE_DETERMINERS_REFCHAIN</th>\n",
       "      <th>PROPORTION_DEMONSTRATIVE_PRONOUNS_REFCHAIN</th>\n",
       "      <th>PROPORTION_REFLEXIVE_PRONOUNS_REFCHAIN</th>\n",
       "      <th>PROPORTION_PROPER_NOUNS_REFCHAIN</th>\n",
       "      <th>AVG_LEN_REFCHAIN</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon-adv.txt</td>\n",
       "      <td>3.36</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.44</td>\n",
       "      <td>4.67</td>\n",
       "      <td>6.10</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon-ele.txt</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2.43</td>\n",
       "      <td>4.56</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon-int.txt</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.43</td>\n",
       "      <td>4.60</td>\n",
       "      <td>5.93</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amsterdam-adv.txt</td>\n",
       "      <td>3.77</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.57</td>\n",
       "      <td>4.43</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>3.545455</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amsterdam-ele.txt</td>\n",
       "      <td>3.82</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.74</td>\n",
       "      <td>4.36</td>\n",
       "      <td>5.51</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>3.692308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>Zero-Hours-ele.txt</td>\n",
       "      <td>3.98</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.55</td>\n",
       "      <td>4.47</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>3.727273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>Zero-Hours-int.txt</td>\n",
       "      <td>4.06</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.52</td>\n",
       "      <td>4.30</td>\n",
       "      <td>5.98</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.61</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>3.615385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>climate-change--adv.txt</td>\n",
       "      <td>3.81</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.45</td>\n",
       "      <td>4.33</td>\n",
       "      <td>5.91</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>climate-change--ele.txt</td>\n",
       "      <td>4.16</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.99</td>\n",
       "      <td>5.40</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>climate-change--int.txt</td>\n",
       "      <td>4.05</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.55</td>\n",
       "      <td>4.02</td>\n",
       "      <td>5.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567 rows × 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Title  AoA_Bird_Lem  AoA_Bristol_Lem  AoA_Cort_Lem  \\\n",
       "0             Amazon-adv.txt          3.36             1.50          2.44   \n",
       "1             Amazon-ele.txt          3.65             1.11          2.43   \n",
       "2             Amazon-int.txt          3.51             1.30          2.43   \n",
       "3          Amsterdam-adv.txt          3.77             1.35          2.57   \n",
       "4          Amsterdam-ele.txt          3.82             1.17          2.74   \n",
       "..                       ...           ...              ...           ...   \n",
       "562       Zero-Hours-ele.txt          3.98             1.48          2.55   \n",
       "563       Zero-Hours-int.txt          4.06             1.40          2.52   \n",
       "564  climate-change--adv.txt          3.81             1.41          2.45   \n",
       "565  climate-change--ele.txt          4.16             1.28          2.68   \n",
       "566  climate-change--int.txt          4.05             1.33          2.55   \n",
       "\n",
       "     AoA_Kup  AoA_Kup_Lem  DISC_RefExprDefArtPerSen  \\\n",
       "0       4.67         6.10                      1.25   \n",
       "1       4.56         5.76                      0.84   \n",
       "2       4.60         5.93                      1.12   \n",
       "3       4.43         5.94                      1.31   \n",
       "4       4.36         5.51                      1.13   \n",
       "..       ...          ...                       ...   \n",
       "562     4.47         5.98                      0.95   \n",
       "563     4.30         5.98                      1.46   \n",
       "564     4.33         5.91                      1.16   \n",
       "565     3.99         5.40                      0.96   \n",
       "566     4.02         5.72                      1.00   \n",
       "\n",
       "     DISC_RefExprDefArtPerWord  DISC_RefExprPerProPerWord  \\\n",
       "0                         0.07                       0.01   \n",
       "1                         0.06                       0.02   \n",
       "2                         0.07                       0.01   \n",
       "3                         0.05                       0.03   \n",
       "4                         0.05                       0.04   \n",
       "..                         ...                        ...   \n",
       "562                       0.04                       0.03   \n",
       "563                       0.05                       0.02   \n",
       "564                       0.05                       0.01   \n",
       "565                       0.05                       0.01   \n",
       "566                       0.05                       0.02   \n",
       "\n",
       "     DISC_RefExprPerPronounsPerSen  ...  PROPORTION_INDEF_NP_REFCHAIN  \\\n",
       "0                             0.22  ...                      0.200000   \n",
       "1                             0.27  ...                      0.115385   \n",
       "2                             0.25  ...                      0.166667   \n",
       "3                             0.89  ...                      0.179487   \n",
       "4                             0.90  ...                      0.208333   \n",
       "..                             ...  ...                           ...   \n",
       "562                           0.60  ...                      0.195122   \n",
       "563                           0.61  ...                      0.170213   \n",
       "564                           0.40  ...                      0.185185   \n",
       "565                           0.27  ...                      0.320000   \n",
       "566                           0.39  ...                      0.233333   \n",
       "\n",
       "     PROPORTION_DEF_NP_REFCHAIN  PROPORTION_PERSONAL_PRONOUNS_REFCHAIN  \\\n",
       "0                      0.485714                               0.200000   \n",
       "1                      0.615385                               0.230769   \n",
       "2                      0.466667                               0.233333   \n",
       "3                      0.461538                               0.358974   \n",
       "4                      0.375000                               0.416667   \n",
       "..                          ...                                    ...   \n",
       "562                    0.268293                               0.317073   \n",
       "563                    0.382979                               0.255319   \n",
       "564                    0.629630                               0.333333   \n",
       "565                    0.480000                               0.320000   \n",
       "566                    0.433333                               0.366667   \n",
       "\n",
       "     PROPORTION_POSSESIVE_DETERMINERS_REFCHAIN  \\\n",
       "0                                          0.0   \n",
       "1                                          0.0   \n",
       "2                                          0.0   \n",
       "3                                          0.0   \n",
       "4                                          0.0   \n",
       "..                                         ...   \n",
       "562                                        0.0   \n",
       "563                                        0.0   \n",
       "564                                        0.0   \n",
       "565                                        0.0   \n",
       "566                                        0.0   \n",
       "\n",
       "     PROPORTION_DEMONSTRATIVE_DETERMINERS_REFCHAIN  \\\n",
       "0                                              0.0   \n",
       "1                                              0.0   \n",
       "2                                              0.0   \n",
       "3                                              0.0   \n",
       "4                                              0.0   \n",
       "..                                             ...   \n",
       "562                                            0.0   \n",
       "563                                            0.0   \n",
       "564                                            0.0   \n",
       "565                                            0.0   \n",
       "566                                            0.0   \n",
       "\n",
       "     PROPORTION_DEMONSTRATIVE_PRONOUNS_REFCHAIN  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "..                                          ...   \n",
       "562                                         0.0   \n",
       "563                                         0.0   \n",
       "564                                         0.0   \n",
       "565                                         0.0   \n",
       "566                                         0.0   \n",
       "\n",
       "     PROPORTION_REFLEXIVE_PRONOUNS_REFCHAIN  PROPORTION_PROPER_NOUNS_REFCHAIN  \\\n",
       "0                                  0.000000                          0.171429   \n",
       "1                                  0.000000                          0.461538   \n",
       "2                                  0.000000                          0.233333   \n",
       "3                                  0.000000                          0.179487   \n",
       "4                                  0.000000                          0.166667   \n",
       "..                                      ...                               ...   \n",
       "562                                0.024390                          0.048780   \n",
       "563                                0.021277                          0.021277   \n",
       "564                                0.000000                          0.222222   \n",
       "565                                0.000000                          0.200000   \n",
       "566                                0.000000                          0.166667   \n",
       "\n",
       "     AVG_LEN_REFCHAIN  Level  \n",
       "0            2.500000      2  \n",
       "1            2.600000      0  \n",
       "2            2.727273      1  \n",
       "3            3.545455      2  \n",
       "4            3.692308      0  \n",
       "..                ...    ...  \n",
       "562          3.727273      0  \n",
       "563          3.615385      1  \n",
       "564          2.700000      2  \n",
       "565          2.777778      0  \n",
       "566          2.727273      1  \n",
       "\n",
       "[567 rows x 157 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_feat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "adv_list = []\n",
    "advanced = '/Users/javm/Desktop/Flatiron4/Reading-Level-Classifier/Texts-SeparatedByReadingLevel/Adv-Txt'\n",
    "for i in (os.listdir(advanced)):\n",
    "    with open(advanced+'/'+i) as f:\n",
    "        lines = f.read()\n",
    "        adv_list.append(lines)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_list_title = list(os.listdir(advanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Swedish prisons have long had a reputation ar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A lonely old man living in a crater on the moo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿Facebook has lost millions of users per month...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿Noise emanating from passing ships may distur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>﻿Scientists have created an “atlas of the brai...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>﻿The forests – and suburbs – of Europe are ech...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>﻿James Hamblin, senior editor of American maga...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>﻿Prince Harry has flown out of Afghanistan at ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>﻿Tigers are more numerous in Nepal than at any...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level\n",
       "0    ﻿It was not so much how hard people found the ...      2\n",
       "1    ﻿Swedish prisons have long had a reputation ar...      2\n",
       "2    A lonely old man living in a crater on the moo...      2\n",
       "3    ﻿Facebook has lost millions of users per month...      2\n",
       "4    ﻿Noise emanating from passing ships may distur...      2\n",
       "..                                                 ...    ...\n",
       "184  ﻿Scientists have created an “atlas of the brai...      2\n",
       "185  ﻿The forests – and suburbs – of Europe are ech...      2\n",
       "186  ﻿James Hamblin, senior editor of American maga...      2\n",
       "187  ﻿Prince Harry has flown out of Afghanistan at ...      2\n",
       "188  ﻿Tigers are more numerous in Nepal than at any...      2\n",
       "\n",
       "[189 rows x 2 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize list of lists\n",
    "data1 = adv_list\n",
    "# Create the pandas DataFrame\n",
    "adv_df = pd.DataFrame(data1, columns = ['Text'] )\n",
    "# print dataframe.\n",
    "adv_df['Level'] = 2\n",
    "adv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WNL Shocking-adv.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Swedish prisons-adv.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WNL John Lewis-adv.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facebook deserted by millions of users-adv.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WNL Ships noise-adv.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>WNL Neuroscientists-adv.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>WNL Brown bears-adv.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>WNL Why we should -adv.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Prince Harry-adv.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Nepal-adv.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Title\n",
       "0                              WNL Shocking-adv.txt\n",
       "1                           Swedish prisons-adv.txt\n",
       "2                            WNL John Lewis-adv.txt\n",
       "3    Facebook deserted by millions of users-adv.txt\n",
       "4                           WNL Ships noise-adv.txt\n",
       "..                                              ...\n",
       "184                     WNL Neuroscientists-adv.txt\n",
       "185                         WNL Brown bears-adv.txt\n",
       "186                      WNL Why we should -adv.txt\n",
       "187                            Prince Harry-adv.txt\n",
       "188                                   Nepal-adv.txt\n",
       "\n",
       "[189 rows x 1 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data11 = adv_list_title\n",
    "# Create the pandas DataFrame\n",
    "adv_title_df = pd.DataFrame(data11, columns = ['Title'] )\n",
    "# print dataframe.\n",
    "adv_title_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WNL Shocking-adv.txt</td>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Swedish prisons-adv.txt</td>\n",
       "      <td>﻿Swedish prisons have long had a reputation ar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WNL John Lewis-adv.txt</td>\n",
       "      <td>A lonely old man living in a crater on the moo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Facebook deserted by millions of users-adv.txt</td>\n",
       "      <td>﻿Facebook has lost millions of users per month...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WNL Ships noise-adv.txt</td>\n",
       "      <td>﻿Noise emanating from passing ships may distur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>WNL Neuroscientists-adv.txt</td>\n",
       "      <td>﻿Scientists have created an “atlas of the brai...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>WNL Brown bears-adv.txt</td>\n",
       "      <td>﻿The forests – and suburbs – of Europe are ech...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>WNL Why we should -adv.txt</td>\n",
       "      <td>﻿James Hamblin, senior editor of American maga...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Prince Harry-adv.txt</td>\n",
       "      <td>﻿Prince Harry has flown out of Afghanistan at ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Nepal-adv.txt</td>\n",
       "      <td>﻿Tigers are more numerous in Nepal than at any...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Title  \\\n",
       "0                              WNL Shocking-adv.txt   \n",
       "1                           Swedish prisons-adv.txt   \n",
       "2                            WNL John Lewis-adv.txt   \n",
       "3    Facebook deserted by millions of users-adv.txt   \n",
       "4                           WNL Ships noise-adv.txt   \n",
       "..                                              ...   \n",
       "184                     WNL Neuroscientists-adv.txt   \n",
       "185                         WNL Brown bears-adv.txt   \n",
       "186                      WNL Why we should -adv.txt   \n",
       "187                            Prince Harry-adv.txt   \n",
       "188                                   Nepal-adv.txt   \n",
       "\n",
       "                                                  Text  Level  \n",
       "0    ﻿It was not so much how hard people found the ...      2  \n",
       "1    ﻿Swedish prisons have long had a reputation ar...      2  \n",
       "2    A lonely old man living in a crater on the moo...      2  \n",
       "3    ﻿Facebook has lost millions of users per month...      2  \n",
       "4    ﻿Noise emanating from passing ships may distur...      2  \n",
       "..                                                 ...    ...  \n",
       "184  ﻿Scientists have created an “atlas of the brai...      2  \n",
       "185  ﻿The forests – and suburbs – of Europe are ech...      2  \n",
       "186  ﻿James Hamblin, senior editor of American maga...      2  \n",
       "187  ﻿Prince Harry has flown out of Afghanistan at ...      2  \n",
       "188  ﻿Tigers are more numerous in Nepal than at any...      2  \n",
       "\n",
       "[189 rows x 3 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_df1 = pd.concat([adv_title_df,adv_df], axis = 1)\n",
    "adv_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_list = []\n",
    "intermediate = '/Users/javm/Desktop/Flatiron4/Reading-Level-Classifier/Texts-SeparatedByReadingLevel/Int-Txt'\n",
    "for i in (os.listdir(intermediate)):\n",
    "    with open(intermediate+'/'+i) as f:\n",
    "        lines = f.read()\n",
    "        int_list.append(lines)\n",
    "        \n",
    "#print(int_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_list_title = list(os.listdir(intermediate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intermediate \\nValdevaqueros is one of the las...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Intermediate \\nThe Duchess of Cambridge gave b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intermediate \\nCathal Redmond was swimming off...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermediate \\nIt has mapped the worlds highes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Intermediate \\nThe customer next to you in the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Intermediate \\nEver since he was diagnosed HIV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Intermediate \\nGalina Zaglumyonova was woken i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Intermediate \\nAll six numbers match, so its t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Intermediate \\nAccording to a recent report, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Intermediate \\nWhen Larry Pizzi first heard ab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level\n",
       "0    Intermediate \\nValdevaqueros is one of the las...      1\n",
       "1    Intermediate \\nThe Duchess of Cambridge gave b...      1\n",
       "2    Intermediate \\nCathal Redmond was swimming off...      1\n",
       "3    Intermediate \\nIt has mapped the worlds highes...      1\n",
       "4    Intermediate \\nThe customer next to you in the...      1\n",
       "..                                                 ...    ...\n",
       "184  Intermediate \\nEver since he was diagnosed HIV...      1\n",
       "185  Intermediate \\nGalina Zaglumyonova was woken i...      1\n",
       "186  Intermediate \\nAll six numbers match, so its t...      1\n",
       "187  Intermediate \\nAccording to a recent report, t...      1\n",
       "188  Intermediate \\nWhen Larry Pizzi first heard ab...      1\n",
       "\n",
       "[189 rows x 2 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize list of lists\n",
    "data3 = int_list\n",
    "# Create the pandas DataFrame\n",
    "int_df = pd.DataFrame(data3, columns = ['Text'] )\n",
    "# print dataframe.\n",
    "int_df['Level'] = 1\n",
    "int_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spain-int.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Royal Baby-int.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WNL The lightweight-int.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arctic mapping-int.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WNL Mystery Shopper-int.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Liberia-int.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Meteorite-int.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Loterry-int.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>WNL India's rich-int.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>WNL Can the US-int.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Title\n",
       "0                  Spain-int.txt\n",
       "1             Royal Baby-int.txt\n",
       "2    WNL The lightweight-int.txt\n",
       "3         Arctic mapping-int.txt\n",
       "4    WNL Mystery Shopper-int.txt\n",
       "..                           ...\n",
       "184              Liberia-int.txt\n",
       "185            Meteorite-int.txt\n",
       "186              Loterry-int.txt\n",
       "187     WNL India's rich-int.txt\n",
       "188       WNL Can the US-int.txt\n",
       "\n",
       "[189 rows x 1 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data22 = int_list_title\n",
    "# Create the pandas DataFrame\n",
    "int_title_df = pd.DataFrame(data22, columns = ['Title'] )\n",
    "# print dataframe.\n",
    "int_title_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spain-int.txt</td>\n",
       "      <td>Intermediate \\nValdevaqueros is one of the las...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Royal Baby-int.txt</td>\n",
       "      <td>Intermediate \\nThe Duchess of Cambridge gave b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WNL The lightweight-int.txt</td>\n",
       "      <td>Intermediate \\nCathal Redmond was swimming off...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arctic mapping-int.txt</td>\n",
       "      <td>Intermediate \\nIt has mapped the worlds highes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WNL Mystery Shopper-int.txt</td>\n",
       "      <td>Intermediate \\nThe customer next to you in the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Liberia-int.txt</td>\n",
       "      <td>Intermediate \\nEver since he was diagnosed HIV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Meteorite-int.txt</td>\n",
       "      <td>Intermediate \\nGalina Zaglumyonova was woken i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Loterry-int.txt</td>\n",
       "      <td>Intermediate \\nAll six numbers match, so its t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>WNL India's rich-int.txt</td>\n",
       "      <td>Intermediate \\nAccording to a recent report, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>WNL Can the US-int.txt</td>\n",
       "      <td>Intermediate \\nWhen Larry Pizzi first heard ab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Title  \\\n",
       "0                  Spain-int.txt   \n",
       "1             Royal Baby-int.txt   \n",
       "2    WNL The lightweight-int.txt   \n",
       "3         Arctic mapping-int.txt   \n",
       "4    WNL Mystery Shopper-int.txt   \n",
       "..                           ...   \n",
       "184              Liberia-int.txt   \n",
       "185            Meteorite-int.txt   \n",
       "186              Loterry-int.txt   \n",
       "187     WNL India's rich-int.txt   \n",
       "188       WNL Can the US-int.txt   \n",
       "\n",
       "                                                  Text  Level  \n",
       "0    Intermediate \\nValdevaqueros is one of the las...      1  \n",
       "1    Intermediate \\nThe Duchess of Cambridge gave b...      1  \n",
       "2    Intermediate \\nCathal Redmond was swimming off...      1  \n",
       "3    Intermediate \\nIt has mapped the worlds highes...      1  \n",
       "4    Intermediate \\nThe customer next to you in the...      1  \n",
       "..                                                 ...    ...  \n",
       "184  Intermediate \\nEver since he was diagnosed HIV...      1  \n",
       "185  Intermediate \\nGalina Zaglumyonova was woken i...      1  \n",
       "186  Intermediate \\nAll six numbers match, so its t...      1  \n",
       "187  Intermediate \\nAccording to a recent report, t...      1  \n",
       "188  Intermediate \\nWhen Larry Pizzi first heard ab...      1  \n",
       "\n",
       "[189 rows x 3 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_df1 = pd.concat([int_title_df,int_df], axis = 1)\n",
    "int_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 189 entries, 0 to 188\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    189 non-null    object\n",
      " 1   Level   189 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.1+ KB\n"
     ]
    }
   ],
   "source": [
    "int_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_list = []\n",
    "ele = '/Users/javm/Desktop/Flatiron4/Reading-Level-Classifier/Texts-SeparatedByReadingLevel/Ele-Txt'\n",
    "for i in (os.listdir(ele)):\n",
    "    with open(ele+'/'+i) as f:\n",
    "        lines = f.read()\n",
    "        ele_list.append(lines)\n",
    "        \n",
    "#print(ele_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_list_title = list(os.listdir(ele))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿SeaWorld’s profits fell by 84% and customers ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Imagine that you read a headline 'Fit in four...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿Robert Mysłajek stops. Between two paw prints...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿The Taliban sent a gunman to shoot Malala You...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿Governments in Europe dream of finding a magi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>﻿Two scientists at Stanford University, in the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>﻿Barack Obama has told young people to reject ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>﻿When two people on a remote Pacific island sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>﻿Google has made maps of the world’s highest m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>﻿Our new international survey, including 33 co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level\n",
       "0    ﻿SeaWorld’s profits fell by 84% and customers ...      0\n",
       "1    ﻿Imagine that you read a headline 'Fit in four...      0\n",
       "2    ﻿Robert Mysłajek stops. Between two paw prints...      0\n",
       "3    ﻿The Taliban sent a gunman to shoot Malala You...      0\n",
       "4    ﻿Governments in Europe dream of finding a magi...      0\n",
       "..                                                 ...    ...\n",
       "184  ﻿Two scientists at Stanford University, in the...      0\n",
       "185  ﻿Barack Obama has told young people to reject ...      0\n",
       "186  ﻿When two people on a remote Pacific island sa...      0\n",
       "187  ﻿Google has made maps of the world’s highest m...      0\n",
       "188  ﻿Our new international survey, including 33 co...      0\n",
       "\n",
       "[189 rows x 2 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize list of lists\n",
    "data2 = ele_list\n",
    "# Create the pandas DataFrame\n",
    "ele_df = pd.DataFrame(data2, columns = ['Text'] )\n",
    "# print dataframe.\n",
    "ele_df['Level'] = 0\n",
    "ele_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WNL SeaWorld-ele.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exercise-ele.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WNL On the trail-ele.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Malala-ele.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WNL Novel way-ele.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>WNL Basic phone logs-ele.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>WNL Barack Obama-ele.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>WNL Castaway-ele.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>WNL Arctic Ramadan-ele.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>WNL Why are people so wrong-ele.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title\n",
       "0                   WNL SeaWorld-ele.txt\n",
       "1                       Exercise-ele.txt\n",
       "2               WNL On the trail-ele.txt\n",
       "3                         Malala-ele.txt\n",
       "4                  WNL Novel way-ele.txt\n",
       "..                                   ...\n",
       "184         WNL Basic phone logs-ele.txt\n",
       "185             WNL Barack Obama-ele.txt\n",
       "186                 WNL Castaway-ele.txt\n",
       "187           WNL Arctic Ramadan-ele.txt\n",
       "188  WNL Why are people so wrong-ele.txt\n",
       "\n",
       "[189 rows x 1 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data33 = ele_list_title\n",
    "# Create the pandas DataFrame\n",
    "ele_title_df = pd.DataFrame(data33, columns = ['Title'] )\n",
    "# print dataframe.\n",
    "ele_title_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WNL SeaWorld-ele.txt</td>\n",
       "      <td>﻿SeaWorld’s profits fell by 84% and customers ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exercise-ele.txt</td>\n",
       "      <td>﻿Imagine that you read a headline 'Fit in four...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WNL On the trail-ele.txt</td>\n",
       "      <td>﻿Robert Mysłajek stops. Between two paw prints...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Malala-ele.txt</td>\n",
       "      <td>﻿The Taliban sent a gunman to shoot Malala You...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WNL Novel way-ele.txt</td>\n",
       "      <td>﻿Governments in Europe dream of finding a magi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>WNL Basic phone logs-ele.txt</td>\n",
       "      <td>﻿Two scientists at Stanford University, in the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>WNL Barack Obama-ele.txt</td>\n",
       "      <td>﻿Barack Obama has told young people to reject ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>WNL Castaway-ele.txt</td>\n",
       "      <td>﻿When two people on a remote Pacific island sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>WNL Arctic Ramadan-ele.txt</td>\n",
       "      <td>﻿Google has made maps of the world’s highest m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>WNL Why are people so wrong-ele.txt</td>\n",
       "      <td>﻿Our new international survey, including 33 co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title  \\\n",
       "0                   WNL SeaWorld-ele.txt   \n",
       "1                       Exercise-ele.txt   \n",
       "2               WNL On the trail-ele.txt   \n",
       "3                         Malala-ele.txt   \n",
       "4                  WNL Novel way-ele.txt   \n",
       "..                                   ...   \n",
       "184         WNL Basic phone logs-ele.txt   \n",
       "185             WNL Barack Obama-ele.txt   \n",
       "186                 WNL Castaway-ele.txt   \n",
       "187           WNL Arctic Ramadan-ele.txt   \n",
       "188  WNL Why are people so wrong-ele.txt   \n",
       "\n",
       "                                                  Text  Level  \n",
       "0    ﻿SeaWorld’s profits fell by 84% and customers ...      0  \n",
       "1    ﻿Imagine that you read a headline 'Fit in four...      0  \n",
       "2    ﻿Robert Mysłajek stops. Between two paw prints...      0  \n",
       "3    ﻿The Taliban sent a gunman to shoot Malala You...      0  \n",
       "4    ﻿Governments in Europe dream of finding a magi...      0  \n",
       "..                                                 ...    ...  \n",
       "184  ﻿Two scientists at Stanford University, in the...      0  \n",
       "185  ﻿Barack Obama has told young people to reject ...      0  \n",
       "186  ﻿When two people on a remote Pacific island sa...      0  \n",
       "187  ﻿Google has made maps of the world’s highest m...      0  \n",
       "188  ﻿Our new international survey, including 33 co...      0  \n",
       "\n",
       "[189 rows x 3 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ele_df1 = pd.concat([ele_title_df,ele_df], axis = 1)\n",
    "ele_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 189 entries, 0 to 188\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    189 non-null    object\n",
      " 1   Level   189 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.1+ KB\n"
     ]
    }
   ],
   "source": [
    "ele_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 378 entries, 0 to 377\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Title   378 non-null    object\n",
      " 1   Text    378 non-null    object\n",
      " 2   Level   378 non-null    int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 11.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df1 = adv_df1.merge(int_df1, how = 'outer')\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = df1.merge(ele_df1, how = 'outer')\n",
    "df2['Title'].replace('-', ' ', regex=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_df['Title'].replace('-', ' ', regex=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>climate change  int.txt</td>\n",
       "      <td>Intermediate \\nLow-income countries will conti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>climate change  ele.txt</td>\n",
       "      <td>﻿Poorer countries will be most affected by cli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>climate change  adv.txt</td>\n",
       "      <td>﻿Low-income countries will remain on the front...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Zero Hours int.txt</td>\n",
       "      <td>Intermediate \\nMore than one million British w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Zero Hours ele.txt</td>\n",
       "      <td>﻿More than one million British workers might b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>Amsterdam ele.txt</td>\n",
       "      <td>﻿To tourists, Amsterdam still seems very liber...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Amsterdam adv.txt</td>\n",
       "      <td>﻿Amsterdam still looks liberal to tourists, wh...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Amazon int.txt</td>\n",
       "      <td>Intermediate\\nWhen you see the word Amazon, wh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>Amazon ele.txt</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Amazon adv.txt</td>\n",
       "      <td>﻿When you see the word Amazon, what’s the firs...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Title  \\\n",
       "342  climate change  int.txt   \n",
       "536  climate change  ele.txt   \n",
       "55   climate change  adv.txt   \n",
       "197       Zero Hours int.txt   \n",
       "398       Zero Hours ele.txt   \n",
       "..                       ...   \n",
       "520        Amsterdam ele.txt   \n",
       "35         Amsterdam adv.txt   \n",
       "320           Amazon int.txt   \n",
       "523           Amazon ele.txt   \n",
       "30            Amazon adv.txt   \n",
       "\n",
       "                                                  Text  Level  \n",
       "342  Intermediate \\nLow-income countries will conti...      1  \n",
       "536  ﻿Poorer countries will be most affected by cli...      0  \n",
       "55   ﻿Low-income countries will remain on the front...      2  \n",
       "197  Intermediate \\nMore than one million British w...      1  \n",
       "398  ﻿More than one million British workers might b...      0  \n",
       "..                                                 ...    ...  \n",
       "520  ﻿To tourists, Amsterdam still seems very liber...      0  \n",
       "35   ﻿Amsterdam still looks liberal to tourists, wh...      2  \n",
       "320  Intermediate\\nWhen you see the word Amazon, wh...      1  \n",
       "523  ﻿When you see the word Amazon, what’s the firs...      0  \n",
       "30   ﻿When you see the word Amazon, what’s the firs...      2  \n",
       "\n",
       "[567 rows x 3 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.sort_values(by = 'Title', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567, 3)\n",
      "(567, 157)\n"
     ]
    }
   ],
   "source": [
    "print(df2.shape)\n",
    "print(new_feat_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df2.merge(new_feat_df, on = 'Title', how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ﻿It was not so much how hard people found the ...\n",
       "1      ﻿Swedish prisons have long had a reputation ar...\n",
       "2      A lonely old man living in a crater on the moo...\n",
       "3      ﻿Facebook has lost millions of users per month...\n",
       "4      ﻿Noise emanating from passing ships may distur...\n",
       "                             ...                        \n",
       "562    ﻿Two scientists at Stanford University, in the...\n",
       "563    ﻿Barack Obama has told young people to reject ...\n",
       "564    ﻿When two people on a remote Pacific island sa...\n",
       "565    ﻿Google has made maps of the world’s highest m...\n",
       "566    ﻿Our new international survey, including 33 co...\n",
       "Name: Text, Length: 558, dtype: object"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = preprocess_pipeline[\n",
    "    'countvec'].get_feature_names()\n",
    "\n",
    "pd.DataFrame(X_tr_proc.toarray(), columns = feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-7bc0fb594e14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvec_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "df[text]\n",
    "\n",
    "vec_text.shape \n",
    "df.shape\n",
    "pd.concat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to downlod punkt to access better tokenization rules\n",
    "# word_tokenize won't work without it\n",
    "#nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = [word_tokenize(doc) for doc in df['Text']]\n",
    "#print(corpus[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import itertools\n",
    "#flattenedcorpus_tokens = pd.Series(list(itertools.chain(*corpus)))\n",
    "#print(flattenedcorpus_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary = pd.Series(flattenedcorpus_tokens.unique())\n",
    "#print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flattenedcorpus_tokens.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_one_occurence = (flattenedcorpus_tokens.\n",
    "                     #value_counts() < 2).sum()\n",
    "#num_one_occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary[dictionary.str.isnumeric()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# imports package with many stopword lists\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get common stop words in english that we'll remove during tokenization/text normalization\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def first_step_normalizer(doc):\n",
    "    # filters for alphabetic (no punctuation or numbers) and filters out stop words. \n",
    "    # lower cases all tokens\n",
    "    #norm_text = [x.lower() for x in word_tokenize(doc) if ((x.isalpha()) & (x.lower() not in stop_words)) ]\n",
    "    #return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df['tok_norm'] = df['Text'].apply(first_step_normalizer)\n",
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_toks_flattened = pd.Series(list(\n",
    "    #itertools.chain(*df['tok_norm'])))\n",
    "#new_dictionary = norm_toks_flattened.unique()\n",
    "#print(len(new_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removed 5k features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_toks_flattened.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/javm/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/javm/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer # lemmatizer using WordNet\n",
    "from nltk.corpus import wordnet # imports WordNet\n",
    "from nltk import pos_tag # nltk's native part of speech tagging\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fully_normalized_corpus = df['Text'].apply(process_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fully_normalized_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flattened_fully_norm = pd.Series(list(itertools.chain(*fully_normalized_corpus)))\n",
    "#len(flattened_fully_norm.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Original dictionary length\n",
    "#print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flattened_fully_norm.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #define attributes to store if text preprocessing requires fitting from data\n",
    "        pass\n",
    "    \n",
    "    def fit(self, data, y = 0):\n",
    "        # this is where you would fit things like corpus specific stopwords\n",
    "        # fit probable bigrams with bigram model in here\n",
    "        \n",
    "        # save as parameters of Text preprocessor\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, data, y = 0):\n",
    "        fully_normalized_corpus = data.apply(self.process_doc)\n",
    "        \n",
    "        return fully_normalized_corpus\n",
    "        \n",
    "    \n",
    "    def process_doc(self, doc):\n",
    "\n",
    "        #initialize lemmatizer\n",
    "        wnl = WordNetLemmatizer()\n",
    "        stop_words = stopwords.words('english')\n",
    "        \n",
    "        # helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "        def pos_tagger(nltk_tag):\n",
    "            if nltk_tag.startswith('J'):\n",
    "                return wordnet.ADJ\n",
    "            elif nltk_tag.startswith('V'):\n",
    "                return wordnet.VERB\n",
    "            elif nltk_tag.startswith('N'):\n",
    "                return wordnet.NOUN\n",
    "            elif nltk_tag.startswith('R'):\n",
    "                return wordnet.ADV\n",
    "            else:         \n",
    "                return None\n",
    "\n",
    "\n",
    "        # remove stop words and punctuations, then lower case\n",
    "        doc_norm = [tok.lower() for tok in word_tokenize(doc) if ((tok.isalpha()) & (tok not in stop_words)) ]\n",
    "\n",
    "        #  POS detection on the result will be important in telling Wordnet's lemmatizer how to lemmatize\n",
    "\n",
    "        # creates list of tuples with tokens and POS tags in wordnet format\n",
    "        wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(doc_norm))) \n",
    "        doc_norm = [wnl.lemmatize(token, pos) for token, pos in wordnet_tagged if pos is not None]\n",
    "\n",
    "        return \" \".join(doc_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = ['Level'])\n",
    "y = df['Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "      <th>AoA_Bird_Lem</th>\n",
       "      <th>AoA_Bristol_Lem</th>\n",
       "      <th>AoA_Cort_Lem</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>AoA_Kup_Lem</th>\n",
       "      <th>DISC_RefExprDefArtPerSen</th>\n",
       "      <th>DISC_RefExprDefArtPerWord</th>\n",
       "      <th>DISC_RefExprPerProPerWord</th>\n",
       "      <th>...</th>\n",
       "      <th>PROPORTION_INDEF_NP_REFCHAIN</th>\n",
       "      <th>PROPORTION_DEF_NP_REFCHAIN</th>\n",
       "      <th>PROPORTION_PERSONAL_PRONOUNS_REFCHAIN</th>\n",
       "      <th>PROPORTION_POSSESIVE_DETERMINERS_REFCHAIN</th>\n",
       "      <th>PROPORTION_DEMONSTRATIVE_DETERMINERS_REFCHAIN</th>\n",
       "      <th>PROPORTION_DEMONSTRATIVE_PRONOUNS_REFCHAIN</th>\n",
       "      <th>PROPORTION_REFLEXIVE_PRONOUNS_REFCHAIN</th>\n",
       "      <th>PROPORTION_PROPER_NOUNS_REFCHAIN</th>\n",
       "      <th>AVG_LEN_REFCHAIN</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.36</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.44</td>\n",
       "      <td>4.67</td>\n",
       "      <td>6.10</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Swedish prisons have long had a reputation ar...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2.43</td>\n",
       "      <td>4.56</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A lonely old man living in a crater on the moo...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.51</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.43</td>\n",
       "      <td>4.60</td>\n",
       "      <td>5.93</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿Facebook has lost millions of users per month...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.77</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.57</td>\n",
       "      <td>4.43</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>3.545455</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿Noise emanating from passing ships may distur...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.82</td>\n",
       "      <td>1.17</td>\n",
       "      <td>2.74</td>\n",
       "      <td>4.36</td>\n",
       "      <td>5.51</td>\n",
       "      <td>1.13</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>3.692308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>﻿Two scientists at Stanford University, in the...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.98</td>\n",
       "      <td>1.48</td>\n",
       "      <td>2.55</td>\n",
       "      <td>4.47</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>3.727273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>﻿Barack Obama has told young people to reject ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.06</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.52</td>\n",
       "      <td>4.30</td>\n",
       "      <td>5.98</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>3.615385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>﻿When two people on a remote Pacific island sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>3.81</td>\n",
       "      <td>1.41</td>\n",
       "      <td>2.45</td>\n",
       "      <td>4.33</td>\n",
       "      <td>5.91</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>﻿Google has made maps of the world’s highest m...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.16</td>\n",
       "      <td>1.28</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.99</td>\n",
       "      <td>5.40</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>﻿Our new international survey, including 33 co...</td>\n",
       "      <td>0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.55</td>\n",
       "      <td>4.02</td>\n",
       "      <td>5.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level  AoA_Bird_Lem  \\\n",
       "0    ﻿It was not so much how hard people found the ...      2          3.36   \n",
       "1    ﻿Swedish prisons have long had a reputation ar...      2          3.65   \n",
       "2    A lonely old man living in a crater on the moo...      2          3.51   \n",
       "3    ﻿Facebook has lost millions of users per month...      2          3.77   \n",
       "4    ﻿Noise emanating from passing ships may distur...      2          3.82   \n",
       "..                                                 ...    ...           ...   \n",
       "562  ﻿Two scientists at Stanford University, in the...      0          3.98   \n",
       "563  ﻿Barack Obama has told young people to reject ...      0          4.06   \n",
       "564  ﻿When two people on a remote Pacific island sa...      0          3.81   \n",
       "565  ﻿Google has made maps of the world’s highest m...      0          4.16   \n",
       "566  ﻿Our new international survey, including 33 co...      0          4.05   \n",
       "\n",
       "     AoA_Bristol_Lem  AoA_Cort_Lem  AoA_Kup  AoA_Kup_Lem  \\\n",
       "0               1.50          2.44     4.67         6.10   \n",
       "1               1.11          2.43     4.56         5.76   \n",
       "2               1.30          2.43     4.60         5.93   \n",
       "3               1.35          2.57     4.43         5.94   \n",
       "4               1.17          2.74     4.36         5.51   \n",
       "..               ...           ...      ...          ...   \n",
       "562             1.48          2.55     4.47         5.98   \n",
       "563             1.40          2.52     4.30         5.98   \n",
       "564             1.41          2.45     4.33         5.91   \n",
       "565             1.28          2.68     3.99         5.40   \n",
       "566             1.33          2.55     4.02         5.72   \n",
       "\n",
       "     DISC_RefExprDefArtPerSen  DISC_RefExprDefArtPerWord  \\\n",
       "0                        1.25                       0.07   \n",
       "1                        0.84                       0.06   \n",
       "2                        1.12                       0.07   \n",
       "3                        1.31                       0.05   \n",
       "4                        1.13                       0.05   \n",
       "..                        ...                        ...   \n",
       "562                      0.95                       0.04   \n",
       "563                      1.46                       0.05   \n",
       "564                      1.16                       0.05   \n",
       "565                      0.96                       0.05   \n",
       "566                      1.00                       0.05   \n",
       "\n",
       "     DISC_RefExprPerProPerWord  ...  PROPORTION_INDEF_NP_REFCHAIN  \\\n",
       "0                         0.01  ...                      0.200000   \n",
       "1                         0.02  ...                      0.115385   \n",
       "2                         0.01  ...                      0.166667   \n",
       "3                         0.03  ...                      0.179487   \n",
       "4                         0.04  ...                      0.208333   \n",
       "..                         ...  ...                           ...   \n",
       "562                       0.03  ...                      0.195122   \n",
       "563                       0.02  ...                      0.170213   \n",
       "564                       0.01  ...                      0.185185   \n",
       "565                       0.01  ...                      0.320000   \n",
       "566                       0.02  ...                      0.233333   \n",
       "\n",
       "     PROPORTION_DEF_NP_REFCHAIN  PROPORTION_PERSONAL_PRONOUNS_REFCHAIN  \\\n",
       "0                      0.485714                               0.200000   \n",
       "1                      0.615385                               0.230769   \n",
       "2                      0.466667                               0.233333   \n",
       "3                      0.461538                               0.358974   \n",
       "4                      0.375000                               0.416667   \n",
       "..                          ...                                    ...   \n",
       "562                    0.268293                               0.317073   \n",
       "563                    0.382979                               0.255319   \n",
       "564                    0.629630                               0.333333   \n",
       "565                    0.480000                               0.320000   \n",
       "566                    0.433333                               0.366667   \n",
       "\n",
       "     PROPORTION_POSSESIVE_DETERMINERS_REFCHAIN  \\\n",
       "0                                          0.0   \n",
       "1                                          0.0   \n",
       "2                                          0.0   \n",
       "3                                          0.0   \n",
       "4                                          0.0   \n",
       "..                                         ...   \n",
       "562                                        0.0   \n",
       "563                                        0.0   \n",
       "564                                        0.0   \n",
       "565                                        0.0   \n",
       "566                                        0.0   \n",
       "\n",
       "     PROPORTION_DEMONSTRATIVE_DETERMINERS_REFCHAIN  \\\n",
       "0                                              0.0   \n",
       "1                                              0.0   \n",
       "2                                              0.0   \n",
       "3                                              0.0   \n",
       "4                                              0.0   \n",
       "..                                             ...   \n",
       "562                                            0.0   \n",
       "563                                            0.0   \n",
       "564                                            0.0   \n",
       "565                                            0.0   \n",
       "566                                            0.0   \n",
       "\n",
       "     PROPORTION_DEMONSTRATIVE_PRONOUNS_REFCHAIN  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "..                                          ...   \n",
       "562                                         0.0   \n",
       "563                                         0.0   \n",
       "564                                         0.0   \n",
       "565                                         0.0   \n",
       "566                                         0.0   \n",
       "\n",
       "     PROPORTION_REFLEXIVE_PRONOUNS_REFCHAIN  PROPORTION_PROPER_NOUNS_REFCHAIN  \\\n",
       "0                                  0.000000                          0.171429   \n",
       "1                                  0.000000                          0.461538   \n",
       "2                                  0.000000                          0.233333   \n",
       "3                                  0.000000                          0.179487   \n",
       "4                                  0.000000                          0.166667   \n",
       "..                                      ...                               ...   \n",
       "562                                0.024390                          0.048780   \n",
       "563                                0.021277                          0.021277   \n",
       "564                                0.000000                          0.222222   \n",
       "565                                0.000000                          0.200000   \n",
       "566                                0.000000                          0.166667   \n",
       "\n",
       "     AVG_LEN_REFCHAIN  Level  \n",
       "0            2.500000      2  \n",
       "1            2.600000      0  \n",
       "2            2.727273      1  \n",
       "3            3.545455      2  \n",
       "4            3.692308      0  \n",
       "..                ...    ...  \n",
       "562          3.727273      0  \n",
       "563          3.615385      1  \n",
       "564          2.700000      2  \n",
       "565          2.777778      0  \n",
       "566          2.727273      1  \n",
       "\n",
       "[567 rows x 158 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X['Text'], y, test_size = 0.3)\n",
    "\n",
    "proc = TextPreprocessor()\n",
    "\n",
    "# again this kind of splitting only becomes important if fitting of text transformers fits to statistics of the text corpus\n",
    "transformed_train = proc.fit_transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      much hard people find challenge far go avoid l...\n",
       "1      prison long reputation world progressive count...\n",
       "2      lonely old man living crater moon unlikely foc...\n",
       "3      lose million user month big market independent...\n",
       "4      emanate pass ship disturb animal killer whale ...\n",
       "                             ...                        \n",
       "562    scientist stanford university usa use metadata...\n",
       "563    obama tell young people reject pessimism meet ...\n",
       "564    people remote pacific island saw small boat wa...\n",
       "565    make map world high mountain ocean floor amazo...\n",
       "566    new international survey include country show ...\n",
       "Name: Text, Length: 558, dtype: object"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prc_steps = [('countvec', CountVectorizer(min_df =.03, max_df = .5))]\n",
    "#preprocess_pipeline = Pipeline(prc_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = df.drop( columns = ['Text','Level'])\n",
    "numeric_transformer = Pipeline(    \n",
    "    steps=[(\"scaler\", StandardScaler())])\n",
    "\n",
    "text_features = df['Text']\n",
    "text_transformer = Pipeline(steps=[\n",
    "    ('countvec', CountVectorizer(min_df =.03, max_df = .5))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"text\", text_transformer, text_features),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-5851430bda81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model score: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[1;32m    329\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    332\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    293\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_remainder\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Make it possible to check for reordered named columns on transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         self._has_str_cols = any(_determine_key_type(cols) == 'str'\n\u001b[0m\u001b[1;32m    320\u001b[0m                                  for cols in self._columns)\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Make it possible to check for reordered named columns on transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         self._has_str_cols = any(_determine_key_type(cols) == 'str'\n\u001b[0m\u001b[1;32m    320\u001b[0m                                  for cols in self._columns)\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'columns'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_determine_key_type\u001b[0;34m(key, accept_slice)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed"
     ]
    }
   ],
   "source": [
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression(random_state=42))]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num_scaled = scaler.fit_transform(X_train_num)\n",
    "\n",
    "X_test_num_scaled = scaler.transform(X_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num_scaled_df = pd.DataFrame(X_train_num_scaled, columns = X_train_num.columns ) \n",
    "X_test_num_scaled_df = pd.DataFrame(X_test_num_scaled, columns = X_test_num.columns ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_tr_proc_df, X_train_num_scaled_df], axis = 1 )\n",
    "\n",
    "test_df = pd.concat([X_test_proc_df, X_test_num_scaled_df], axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-251-112b8dcce5f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m feat_names = preprocess_pipeline[\n\u001b[0m\u001b[1;32m      2\u001b[0m     'countvec'].get_feature_names()\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr_proc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \"\"\"\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m         return [t for t, i in sorted(self.vocabulary_.items(),\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "feat_names = preprocess_pipeline[\n",
    "    'countvec'].get_feature_names()\n",
    "\n",
    "pd.DataFrame(X_tr_proc2.toarray(), columns = feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>AoA_Bird_Lem</th>\n",
       "      <th>AoA_Bristol_Lem</th>\n",
       "      <th>AoA_Cort_Lem</th>\n",
       "      <th>AoA_Kup</th>\n",
       "      <th>AoA_Kup_Lem</th>\n",
       "      <th>DISC_RefExprDefArtPerSen</th>\n",
       "      <th>DISC_RefExprDefArtPerWord</th>\n",
       "      <th>DISC_RefExprPerProPerWord</th>\n",
       "      <th>DISC_RefExprPerPronounsPerSen</th>\n",
       "      <th>...</th>\n",
       "      <th>AVG_NUM_WORD_PER_ENTITY</th>\n",
       "      <th>PROPORTION_INDEF_NP_REFCHAIN</th>\n",
       "      <th>PROPORTION_DEF_NP_REFCHAIN</th>\n",
       "      <th>PROPORTION_PERSONAL_PRONOUNS_REFCHAIN</th>\n",
       "      <th>PROPORTION_POSSESIVE_DETERMINERS_REFCHAIN</th>\n",
       "      <th>PROPORTION_DEMONSTRATIVE_DETERMINERS_REFCHAIN</th>\n",
       "      <th>PROPORTION_DEMONSTRATIVE_PRONOUNS_REFCHAIN</th>\n",
       "      <th>PROPORTION_REFLEXIVE_PRONOUNS_REFCHAIN</th>\n",
       "      <th>PROPORTION_PROPER_NOUNS_REFCHAIN</th>\n",
       "      <th>AVG_LEN_REFCHAIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 735)\\t5\\n  (0, 10)\\t5\\n  (0, 695)\\t1\\n  ...</td>\n",
       "      <td>0.637511</td>\n",
       "      <td>1.560606</td>\n",
       "      <td>-1.627561</td>\n",
       "      <td>2.429856</td>\n",
       "      <td>1.127638</td>\n",
       "      <td>-0.553891</td>\n",
       "      <td>-0.355282</td>\n",
       "      <td>-0.699945</td>\n",
       "      <td>-0.555624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.709873</td>\n",
       "      <td>0.093698</td>\n",
       "      <td>-0.695000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.416693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.455915</td>\n",
       "      <td>-1.224623</td>\n",
       "      <td>-0.658360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 204)\\t2\\n  (0, 278)\\t1\\n  (0, 550)\\t1\\n ...</td>\n",
       "      <td>0.637511</td>\n",
       "      <td>-1.420236</td>\n",
       "      <td>1.447045</td>\n",
       "      <td>-1.079433</td>\n",
       "      <td>-1.458029</td>\n",
       "      <td>-0.387598</td>\n",
       "      <td>-0.355282</td>\n",
       "      <td>1.345651</td>\n",
       "      <td>1.431411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.358099</td>\n",
       "      <td>-0.417934</td>\n",
       "      <td>1.349245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.416693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.552395</td>\n",
       "      <td>-0.268286</td>\n",
       "      <td>3.661622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 176)\\t2\\n  (0, 471)\\t1\\n  (0, 137)\\t1\\n ...</td>\n",
       "      <td>-0.390179</td>\n",
       "      <td>-0.202709</td>\n",
       "      <td>0.348971</td>\n",
       "      <td>-0.720528</td>\n",
       "      <td>0.804430</td>\n",
       "      <td>1.370361</td>\n",
       "      <td>1.410724</td>\n",
       "      <td>-0.699945</td>\n",
       "      <td>-0.407338</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.849582</td>\n",
       "      <td>0.842609</td>\n",
       "      <td>-0.555505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.416693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.455915</td>\n",
       "      <td>0.497821</td>\n",
       "      <td>-0.343891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 238)\\t4\\n  (0, 254)\\t4\\n  (0, 639)\\t1\\n ...</td>\n",
       "      <td>0.101325</td>\n",
       "      <td>-0.790481</td>\n",
       "      <td>0.898008</td>\n",
       "      <td>-0.561015</td>\n",
       "      <td>-1.425708</td>\n",
       "      <td>0.728943</td>\n",
       "      <td>0.233386</td>\n",
       "      <td>-0.699945</td>\n",
       "      <td>-0.348023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.041200</td>\n",
       "      <td>1.929701</td>\n",
       "      <td>-0.576249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.416693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.455915</td>\n",
       "      <td>1.361237</td>\n",
       "      <td>-0.163362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 306)\\t1\\n  (0, 542)\\t1\\n  (0, 378)\\t1\\n ...</td>\n",
       "      <td>2.335432</td>\n",
       "      <td>-0.706513</td>\n",
       "      <td>-1.188332</td>\n",
       "      <td>0.236551</td>\n",
       "      <td>0.513542</td>\n",
       "      <td>-1.337846</td>\n",
       "      <td>-1.532620</td>\n",
       "      <td>0.834252</td>\n",
       "      <td>0.423065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.853324</td>\n",
       "      <td>-0.080879</td>\n",
       "      <td>0.717092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.229984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.455915</td>\n",
       "      <td>-0.648467</td>\n",
       "      <td>-0.580713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>(0, 424)\\t1\\n  (0, 388)\\t1\\n  (0, 204)\\t1\\n ...</td>\n",
       "      <td>0.146007</td>\n",
       "      <td>-0.496595</td>\n",
       "      <td>0.788201</td>\n",
       "      <td>0.715090</td>\n",
       "      <td>-1.005537</td>\n",
       "      <td>0.990262</td>\n",
       "      <td>1.410724</td>\n",
       "      <td>-0.188546</td>\n",
       "      <td>-0.140423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.155091</td>\n",
       "      <td>0.749940</td>\n",
       "      <td>-0.826882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.416693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.455915</td>\n",
       "      <td>1.245547</td>\n",
       "      <td>-0.232991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>(0, 254)\\t2\\n  (0, 278)\\t3\\n  (0, 639)\\t1\\n ...</td>\n",
       "      <td>0.369418</td>\n",
       "      <td>-1.126350</td>\n",
       "      <td>1.117623</td>\n",
       "      <td>-0.002719</td>\n",
       "      <td>-0.714650</td>\n",
       "      <td>-1.337846</td>\n",
       "      <td>-1.532620</td>\n",
       "      <td>1.857051</td>\n",
       "      <td>1.757641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.695950</td>\n",
       "      <td>-1.564785</td>\n",
       "      <td>1.475375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.416693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.455915</td>\n",
       "      <td>-1.173571</td>\n",
       "      <td>-0.425420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>(0, 363)\\t1\\n  (0, 471)\\t1\\n  (0, 137)\\t1\\n ...</td>\n",
       "      <td>0.235371</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>-0.858910</td>\n",
       "      <td>0.435942</td>\n",
       "      <td>-0.068233</td>\n",
       "      <td>-0.197548</td>\n",
       "      <td>-0.355282</td>\n",
       "      <td>-0.188546</td>\n",
       "      <td>-0.051451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.954162</td>\n",
       "      <td>-0.080879</td>\n",
       "      <td>0.868749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.229984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.455915</td>\n",
       "      <td>-0.156181</td>\n",
       "      <td>-0.580713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>(0, 542)\\t1\\n  (0, 471)\\t1\\n  (0, 380)\\t1\\n ...</td>\n",
       "      <td>0.860921</td>\n",
       "      <td>0.636965</td>\n",
       "      <td>-0.913814</td>\n",
       "      <td>-0.002719</td>\n",
       "      <td>0.448901</td>\n",
       "      <td>-0.031255</td>\n",
       "      <td>0.233386</td>\n",
       "      <td>-0.188546</td>\n",
       "      <td>-0.140423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.850877</td>\n",
       "      <td>-0.006060</td>\n",
       "      <td>0.262122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.217991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.455915</td>\n",
       "      <td>-0.320276</td>\n",
       "      <td>0.302518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>(0, 695)\\t2\\n  (0, 136)\\t1\\n  (0, 83)\\t2\\n  ...</td>\n",
       "      <td>1.307743</td>\n",
       "      <td>-0.034774</td>\n",
       "      <td>1.611756</td>\n",
       "      <td>-0.680650</td>\n",
       "      <td>0.319617</td>\n",
       "      <td>-0.078767</td>\n",
       "      <td>-0.355282</td>\n",
       "      <td>-1.211345</td>\n",
       "      <td>-1.208083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.575451</td>\n",
       "      <td>0.326468</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.416693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.455915</td>\n",
       "      <td>-0.714105</td>\n",
       "      <td>-0.134244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>396 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  AoA_Bird_Lem  \\\n",
       "0      (0, 735)\\t5\\n  (0, 10)\\t5\\n  (0, 695)\\t1\\n  ...      0.637511   \n",
       "1      (0, 204)\\t2\\n  (0, 278)\\t1\\n  (0, 550)\\t1\\n ...      0.637511   \n",
       "2      (0, 176)\\t2\\n  (0, 471)\\t1\\n  (0, 137)\\t1\\n ...     -0.390179   \n",
       "3      (0, 238)\\t4\\n  (0, 254)\\t4\\n  (0, 639)\\t1\\n ...      0.101325   \n",
       "4      (0, 306)\\t1\\n  (0, 542)\\t1\\n  (0, 378)\\t1\\n ...      2.335432   \n",
       "..                                                 ...           ...   \n",
       "391    (0, 424)\\t1\\n  (0, 388)\\t1\\n  (0, 204)\\t1\\n ...      0.146007   \n",
       "392    (0, 254)\\t2\\n  (0, 278)\\t3\\n  (0, 639)\\t1\\n ...      0.369418   \n",
       "393    (0, 363)\\t1\\n  (0, 471)\\t1\\n  (0, 137)\\t1\\n ...      0.235371   \n",
       "394    (0, 542)\\t1\\n  (0, 471)\\t1\\n  (0, 380)\\t1\\n ...      0.860921   \n",
       "395    (0, 695)\\t2\\n  (0, 136)\\t1\\n  (0, 83)\\t2\\n  ...      1.307743   \n",
       "\n",
       "     AoA_Bristol_Lem  AoA_Cort_Lem   AoA_Kup  AoA_Kup_Lem  \\\n",
       "0           1.560606     -1.627561  2.429856     1.127638   \n",
       "1          -1.420236      1.447045 -1.079433    -1.458029   \n",
       "2          -0.202709      0.348971 -0.720528     0.804430   \n",
       "3          -0.790481      0.898008 -0.561015    -1.425708   \n",
       "4          -0.706513     -1.188332  0.236551     0.513542   \n",
       "..               ...           ...       ...          ...   \n",
       "391        -0.496595      0.788201  0.715090    -1.005537   \n",
       "392        -1.126350      1.117623 -0.002719    -0.714650   \n",
       "393         0.007209     -0.858910  0.435942    -0.068233   \n",
       "394         0.636965     -0.913814 -0.002719     0.448901   \n",
       "395        -0.034774      1.611756 -0.680650     0.319617   \n",
       "\n",
       "     DISC_RefExprDefArtPerSen  DISC_RefExprDefArtPerWord  \\\n",
       "0                   -0.553891                  -0.355282   \n",
       "1                   -0.387598                  -0.355282   \n",
       "2                    1.370361                   1.410724   \n",
       "3                    0.728943                   0.233386   \n",
       "4                   -1.337846                  -1.532620   \n",
       "..                        ...                        ...   \n",
       "391                  0.990262                   1.410724   \n",
       "392                 -1.337846                  -1.532620   \n",
       "393                 -0.197548                  -0.355282   \n",
       "394                 -0.031255                   0.233386   \n",
       "395                 -0.078767                  -0.355282   \n",
       "\n",
       "     DISC_RefExprPerProPerWord  DISC_RefExprPerPronounsPerSen  ...  \\\n",
       "0                    -0.699945                      -0.555624  ...   \n",
       "1                     1.345651                       1.431411  ...   \n",
       "2                    -0.699945                      -0.407338  ...   \n",
       "3                    -0.699945                      -0.348023  ...   \n",
       "4                     0.834252                       0.423065  ...   \n",
       "..                         ...                            ...  ...   \n",
       "391                  -0.188546                      -0.140423  ...   \n",
       "392                   1.857051                       1.757641  ...   \n",
       "393                  -0.188546                      -0.051451  ...   \n",
       "394                  -0.188546                      -0.140423  ...   \n",
       "395                  -1.211345                      -1.208083  ...   \n",
       "\n",
       "     AVG_NUM_WORD_PER_ENTITY  PROPORTION_INDEF_NP_REFCHAIN  \\\n",
       "0                        0.0                      0.709873   \n",
       "1                        0.0                     -1.358099   \n",
       "2                        0.0                      0.849582   \n",
       "3                        0.0                     -1.041200   \n",
       "4                        0.0                      0.853324   \n",
       "..                       ...                           ...   \n",
       "391                      0.0                      1.155091   \n",
       "392                      0.0                     -0.695950   \n",
       "393                      0.0                     -0.954162   \n",
       "394                      0.0                     -0.850877   \n",
       "395                      0.0                     -0.575451   \n",
       "\n",
       "     PROPORTION_DEF_NP_REFCHAIN  PROPORTION_PERSONAL_PRONOUNS_REFCHAIN  \\\n",
       "0                      0.093698                              -0.695000   \n",
       "1                     -0.417934                               1.349245   \n",
       "2                      0.842609                              -0.555505   \n",
       "3                      1.929701                              -0.576249   \n",
       "4                     -0.080879                               0.717092   \n",
       "..                          ...                                    ...   \n",
       "391                    0.749940                              -0.826882   \n",
       "392                   -1.564785                               1.475375   \n",
       "393                   -0.080879                               0.868749   \n",
       "394                   -0.006060                               0.262122   \n",
       "395                    0.326468                              -0.506271   \n",
       "\n",
       "     PROPORTION_POSSESIVE_DETERMINERS_REFCHAIN  \\\n",
       "0                                          0.0   \n",
       "1                                          0.0   \n",
       "2                                          0.0   \n",
       "3                                          0.0   \n",
       "4                                          0.0   \n",
       "..                                         ...   \n",
       "391                                        0.0   \n",
       "392                                        0.0   \n",
       "393                                        0.0   \n",
       "394                                        0.0   \n",
       "395                                        0.0   \n",
       "\n",
       "     PROPORTION_DEMONSTRATIVE_DETERMINERS_REFCHAIN  \\\n",
       "0                                        -0.416693   \n",
       "1                                        -0.416693   \n",
       "2                                        -0.416693   \n",
       "3                                        -0.416693   \n",
       "4                                         1.229984   \n",
       "..                                             ...   \n",
       "391                                      -0.416693   \n",
       "392                                      -0.416693   \n",
       "393                                       1.229984   \n",
       "394                                       2.217991   \n",
       "395                                      -0.416693   \n",
       "\n",
       "     PROPORTION_DEMONSTRATIVE_PRONOUNS_REFCHAIN  \\\n",
       "0                                           0.0   \n",
       "1                                           0.0   \n",
       "2                                           0.0   \n",
       "3                                           0.0   \n",
       "4                                           0.0   \n",
       "..                                          ...   \n",
       "391                                         0.0   \n",
       "392                                         0.0   \n",
       "393                                         0.0   \n",
       "394                                         0.0   \n",
       "395                                         0.0   \n",
       "\n",
       "     PROPORTION_REFLEXIVE_PRONOUNS_REFCHAIN  PROPORTION_PROPER_NOUNS_REFCHAIN  \\\n",
       "0                                 -0.455915                         -1.224623   \n",
       "1                                  0.552395                         -0.268286   \n",
       "2                                 -0.455915                          0.497821   \n",
       "3                                 -0.455915                          1.361237   \n",
       "4                                 -0.455915                         -0.648467   \n",
       "..                                      ...                               ...   \n",
       "391                               -0.455915                          1.245547   \n",
       "392                               -0.455915                         -1.173571   \n",
       "393                               -0.455915                         -0.156181   \n",
       "394                               -0.455915                         -0.320276   \n",
       "395                               -0.455915                         -0.714105   \n",
       "\n",
       "     AVG_LEN_REFCHAIN  \n",
       "0           -0.658360  \n",
       "1            3.661622  \n",
       "2           -0.343891  \n",
       "3           -0.163362  \n",
       "4           -0.580713  \n",
       "..                ...  \n",
       "391         -0.232991  \n",
       "392         -0.425420  \n",
       "393         -0.580713  \n",
       "394          0.302518  \n",
       "395         -0.134244  \n",
       "\n",
       "[396 rows x 156 columns]"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Wrong number of items passed 2, placement implies 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Level'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3573\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3574\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3575\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Level'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-623-a135b31d5a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbow_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeat_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbow_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Level'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbow_mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3038\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3039\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3040\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3116\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3117\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3119\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3575\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3576\u001b[0m             \u001b[0;31m# This item wasn't present, just insert at end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3577\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3578\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m         \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblkno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_fast_count_smallints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblknos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   2720\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatetimeArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2722\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_ndim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 2, placement implies 1"
     ]
    }
   ],
   "source": [
    "bow_mat = pd.DataFrame(X_tr_proc.toarray(), columns = feat_names)\n",
    "bow_mat['Level'] = y_train\n",
    "bow_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bow_mat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-b139a8a110c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclass2_bow_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbow_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Level'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Level'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# class 1 token probabilities:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mN_tok_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass2_bow_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mN_2\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mclass2_bow_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bow_mat' is not defined"
     ]
    }
   ],
   "source": [
    "class2_bow_mat = bow_mat[bow_mat['Level'] == 2].drop(columns = ['Level'])\n",
    "\n",
    "# class 1 token probabilities:\n",
    "N_tok_2 = class2_bow_mat.sum(axis = 0) \n",
    "N_2 =  class2_bow_mat.values.sum()\n",
    "\n",
    "# get probabilities for each token: class 1\n",
    "proba_c2 = N_tok_2/N_2\n",
    "\n",
    "proba_c2.sort_values(ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1_bow_mat = bow_mat[bow_mat['Level'] == 1].drop(columns = ['Level'])\n",
    "\n",
    "# class 1 token probabilities:\n",
    "N_tok_1 = class1_bow_mat.sum(axis = 0) \n",
    "N_1 =  class1_bow_mat.values.sum()\n",
    "\n",
    "# get probabilities for each token: class 1\n",
    "proba_c1 = N_tok_1/N_1\n",
    "\n",
    "proba_c1.sort_values(ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class0_bow_mat = bow_mat[bow_mat['Level'] == 0].drop(columns = ['Level'])\n",
    "\n",
    "# class 1 token probabilities:\n",
    "N_tok_0 = class0_bow_mat.sum(axis = 0)\n",
    "N_0 =  class0_bow_mat.values.sum() \n",
    "\n",
    "# get probabilities for each token: class 0\n",
    "proba_c0 = N_tok_0/N_0\n",
    "\n",
    "proba_c0.sort_values(ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Multinomial Naive Bayes Classifier to pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "mod_pipe = deepcopy(preprocess_pipeline)\n",
    "mod_pipe.steps.append(('multinb', MultinomialNB()))\n",
    "mod_pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_pipe.fit(transformed_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test = proc.transform(X_test)\n",
    "\n",
    "y_pred = mod_pipe.predict(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOOK HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameter range \n",
    "param_grid = {'C': [0.1, 1, 10, 100],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              #'gamma':['scale', 'auto'],\n",
    "              'kernel': ['linear']}  \n",
    "   \n",
    "grid = GridSearchCV(SVC(random_state = 42), param_grid, refit = True, verbose = 3,n_jobs=-1) \n",
    "   \n",
    "# fitting the model for grid search \n",
    "grid.fit(X_tr_proc, y_train) \n",
    " \n",
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "grid_predictions = grid.predict(X_test_proc) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(y_test, grid_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFID Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_proc2 = preprocess_pipeline2.fit_transform(transformed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_steps2 = [('tfid', TfidfVectorizer(min_df = 0.05, max_df = 0.95))]\n",
    "preprocess_pipeline2 = Pipeline(prc_steps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(567, 543)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_names = preprocess_pipeline2[\n",
    "    'tfid'].get_feature_names()\n",
    "\n",
    "word_vec = pd.DataFrame(X_tr_proc2.toarray(), columns = feat_names)\n",
    "pd.concat([df, word_vec],1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_proc2 = preprocess_pipeline2.transform(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameter range \n",
    "param_grid = {'C': [0.1, 1, 10, 100],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              #'gamma':['scale', 'auto'],\n",
    "              'kernel': ['linear']}  \n",
    "   \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3,n_jobs=-1) \n",
    "   \n",
    "# fitting the model for grid search \n",
    "grid.fit(X_tr_proc2, y_train) \n",
    " \n",
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "grid_predictions2 = grid.predict(X_test_proc2) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(y_test, grid_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    " \n",
    "# GaussianNB is used in Binomial Classification\n",
    "# MultinomialNB is used in multi-class classification\n",
    "#clf = GaussianNB()\n",
    "clf = MultinomialNB()\n",
    " \n",
    "# Printing all the parameters of Naive Bayes\n",
    "print(clf)\n",
    " \n",
    "NB=clf.fit(X_tr_proc,y_train)\n",
    "prediction=NB.predict(X_test_proc)\n",
    " \n",
    "# Measuring accuracy on Testing Data\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(metrics.confusion_matrix(y_test, prediction))\n",
    " \n",
    "# Printing the Overall Accuracy of the model\n",
    "F1_Score=metrics.f1_score(y_test, prediction, average='weighted')\n",
    "print('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n",
    " \n",
    "# Importing cross validation function from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "# Running 10-Fold Cross validation on a given algorithm\n",
    "# Passing full data X and y because the K-fold will split the data and automatically choose train/test\n",
    "Accuracy_Values=cross_val_score(NB, X , y, cv=5, scoring='f1_weighted')\n",
    "print('\\nAccuracy values for 5-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\n",
    " \n",
    "# GaussianNB is used in Binomial Classification\n",
    "# MultinomialNB is used in multi-class classification\n",
    "#clf = GaussianNB()\n",
    "clf = ComplementNB()\n",
    " \n",
    "# Printing all the parameters of Naive Bayes\n",
    "print(clf)\n",
    " \n",
    "NB=clf.fit(X_tr_proc2,y_train)\n",
    "prediction=NB.predict(X_test_proc2)\n",
    " \n",
    "# Measuring accuracy on Testing Data\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(metrics.confusion_matrix(y_test, prediction))\n",
    " \n",
    "# Printing the Overall Accuracy of the model\n",
    "F1_Score=metrics.f1_score(y_test, prediction, average='weighted')\n",
    "print('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n",
    " \n",
    "# Importing cross validation function from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "# Running 10-Fold Cross validation on a given algorithm\n",
    "# Passing full data X and y because the K-fold will split the data and automatically choose train/test\n",
    "Accuracy_Values=cross_val_score(NB, X , y, cv=5, scoring='f1_weighted')\n",
    "print('\\nAccuracy values for 5-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
