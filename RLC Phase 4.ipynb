{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Level Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/javm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffIt was not so much how hard people found the challenge but how far they would go to avoid it that left researchers gobsmacked. The task? To sit in a chair and do nothing but think. \\nSo unbearable did some find it that they took up the safe but alarming opportunity to give themselves mild electric shocks in an attempt to break the tedium. \\nTwo-thirds of men pressed a button to deliver a painful jolt during a 15-minute spell of solitude. \\nUnder the same conditions, a quarter of women pressed the shock button. The difference, scientists suspect, is that men tend to be more sensation-seeking than women. \\nThe report from psychologists at Virginia and Harvard Universities is one of a surprising few to tackle the question of why most of us find it so hard to do nothing.',\n",
       " '\\ufeffSwedish prisons have long had a reputation around the world for being progressive. But are the country’s prisons a soft option? \\nThe head of Sweden’s prison and probation service, Nils Oberg, announced in November 2013 that four Swedish prisons are to be closed due to an “out of the ordinary” decline in prisoner numbers. \\nAlthough there has been no fall in crime rates, between 2011 and 2012 there was a 6% drop in Sweden’s prisoner population, now a little over 4,500. A similar decrease is expected in 2013 and 2014. Oberg admitted to being puzzled by the unexpected dip, but expressed optimism that the reason was to do with how his prisons are run. “We certainly hope that the efforts we invest in rehabilitation and preventing relapse of crime has had an impact,” he said. \\n“The modern prison service in Sweden is very different from when I joined as a young prison officer in 1978,” says Kenneth Gustafsson, governor of Kumla Prison, Sweden’s most secure jail, situated 130 miles west of Stockholm. However, he doesn’t think the system has gone soft. “When I joined, the focus was very much on humanity in prisons. Prisoners were treated well – maybe too well, some might say. But, after a number of high-profile escapes in 2004, we had to rebalance and place more emphasis on security.” \\nDespite the hardening of attitudes toward prison security following the escape scandals, the Swedes still managed to maintain a broadly humane approach to sentencing, even of the most serious offenders: jail terms rarely exceed ten years; those who receive life imprisonment can still apply to the courts after a decade to have the sentence commuted to a fixed term, usually in the region of 18 to 25 years. Sweden was the first country in Europe to introduce the electronic tagging of convicted criminals and continues to strive to minimize short-term prison sentences wherever possible by using community-based measures, which have been proven to be more effective at reducing reoffending.']"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_list = []\n",
    "advanced = '/Users/javm/Desktop/Flatiron4/Reading-Level-Classifier/Texts-SeparatedByReadingLevel/Adv-Txt'\n",
    "for i in (os.listdir(advanced)):\n",
    "    with open(advanced+'/'+i) as f:\n",
    "        lines = f.read()\n",
    "        adv_list.append(lines)\n",
    "        \n",
    "adv_list[0:2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Swedish prisons have long had a reputation ar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A lonely old man living in a crater on the moo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿Facebook has lost millions of users per month...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿Noise emanating from passing ships may distur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>﻿Scientists have created an “atlas of the brai...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>﻿The forests – and suburbs – of Europe are ech...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>﻿James Hamblin, senior editor of American maga...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>﻿Prince Harry has flown out of Afghanistan at ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>﻿Tigers are more numerous in Nepal than at any...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level\n",
       "0    ﻿It was not so much how hard people found the ...      2\n",
       "1    ﻿Swedish prisons have long had a reputation ar...      2\n",
       "2    A lonely old man living in a crater on the moo...      2\n",
       "3    ﻿Facebook has lost millions of users per month...      2\n",
       "4    ﻿Noise emanating from passing ships may distur...      2\n",
       "..                                                 ...    ...\n",
       "184  ﻿Scientists have created an “atlas of the brai...      2\n",
       "185  ﻿The forests – and suburbs – of Europe are ech...      2\n",
       "186  ﻿James Hamblin, senior editor of American maga...      2\n",
       "187  ﻿Prince Harry has flown out of Afghanistan at ...      2\n",
       "188  ﻿Tigers are more numerous in Nepal than at any...      2\n",
       "\n",
       "[189 rows x 2 columns]"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize list of lists\n",
    "data1 = adv_list\n",
    "# Create the pandas DataFrame\n",
    "adv_df = pd.DataFrame(data1, columns = ['Text'] )\n",
    "# print dataframe.\n",
    "adv_df['Level'] = 2\n",
    "adv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_list = []\n",
    "intermediate = '/Users/javm/Desktop/Flatiron4/Reading-Level-Classifier/Texts-SeparatedByReadingLevel/Int-Txt'\n",
    "for i in (os.listdir(intermediate)):\n",
    "    with open(intermediate+'/'+i) as f:\n",
    "        lines = f.read()\n",
    "        int_list.append(lines)\n",
    "        \n",
    "#print(int_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intermediate \\nValdevaqueros is one of the las...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Intermediate \\nThe Duchess of Cambridge gave b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Intermediate \\nCathal Redmond was swimming off...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermediate \\nIt has mapped the worlds highes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Intermediate \\nThe customer next to you in the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Intermediate \\nEver since he was diagnosed HIV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Intermediate \\nGalina Zaglumyonova was woken i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Intermediate \\nAll six numbers match, so its t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Intermediate \\nAccording to a recent report, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Intermediate \\nWhen Larry Pizzi first heard ab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level\n",
       "0    Intermediate \\nValdevaqueros is one of the las...      1\n",
       "1    Intermediate \\nThe Duchess of Cambridge gave b...      1\n",
       "2    Intermediate \\nCathal Redmond was swimming off...      1\n",
       "3    Intermediate \\nIt has mapped the worlds highes...      1\n",
       "4    Intermediate \\nThe customer next to you in the...      1\n",
       "..                                                 ...    ...\n",
       "184  Intermediate \\nEver since he was diagnosed HIV...      1\n",
       "185  Intermediate \\nGalina Zaglumyonova was woken i...      1\n",
       "186  Intermediate \\nAll six numbers match, so its t...      1\n",
       "187  Intermediate \\nAccording to a recent report, t...      1\n",
       "188  Intermediate \\nWhen Larry Pizzi first heard ab...      1\n",
       "\n",
       "[189 rows x 2 columns]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize list of lists\n",
    "data3 = int_list\n",
    "# Create the pandas DataFrame\n",
    "int_df = pd.DataFrame(data3, columns = ['Text'] )\n",
    "# print dataframe.\n",
    "int_df['Level'] = 1\n",
    "int_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_list = []\n",
    "ele = '/Users/javm/Desktop/Flatiron4/Reading-Level-Classifier/Texts-SeparatedByReadingLevel/Ele-Txt'\n",
    "for i in (os.listdir(ele)):\n",
    "    with open(ele+'/'+i) as f:\n",
    "        lines = f.read()\n",
    "        ele_list.append(lines)\n",
    "        \n",
    "#print(ele_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿SeaWorld’s profits fell by 84% and customers ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Imagine that you read a headline 'Fit in four...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>﻿Robert Mysłajek stops. Between two paw prints...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿The Taliban sent a gunman to shoot Malala You...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿Governments in Europe dream of finding a magi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>﻿Two scientists at Stanford University, in the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>﻿Barack Obama has told young people to reject ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>﻿When two people on a remote Pacific island sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>﻿Google has made maps of the world’s highest m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>﻿Our new international survey, including 33 co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level\n",
       "0    ﻿SeaWorld’s profits fell by 84% and customers ...      0\n",
       "1    ﻿Imagine that you read a headline 'Fit in four...      0\n",
       "2    ﻿Robert Mysłajek stops. Between two paw prints...      0\n",
       "3    ﻿The Taliban sent a gunman to shoot Malala You...      0\n",
       "4    ﻿Governments in Europe dream of finding a magi...      0\n",
       "..                                                 ...    ...\n",
       "184  ﻿Two scientists at Stanford University, in the...      0\n",
       "185  ﻿Barack Obama has told young people to reject ...      0\n",
       "186  ﻿When two people on a remote Pacific island sa...      0\n",
       "187  ﻿Google has made maps of the world’s highest m...      0\n",
       "188  ﻿Our new international survey, including 33 co...      0\n",
       "\n",
       "[189 rows x 2 columns]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize list of lists\n",
    "data2 = ele_list\n",
    "# Create the pandas DataFrame\n",
    "ele_df = pd.DataFrame(data2, columns = ['Text'] )\n",
    "# print dataframe.\n",
    "ele_df['Level'] = 0\n",
    "ele_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Swedish prisons have long had a reputation ar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A lonely old man living in a crater on the moo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿Facebook has lost millions of users per month...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿Noise emanating from passing ships may distur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>Intermediate \\nEver since he was diagnosed HIV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>Intermediate \\nGalina Zaglumyonova was woken i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Intermediate \\nAll six numbers match, so its t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>Intermediate \\nAccording to a recent report, t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>Intermediate \\nWhen Larry Pizzi first heard ab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>378 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level\n",
       "0    ﻿It was not so much how hard people found the ...      2\n",
       "1    ﻿Swedish prisons have long had a reputation ar...      2\n",
       "2    A lonely old man living in a crater on the moo...      2\n",
       "3    ﻿Facebook has lost millions of users per month...      2\n",
       "4    ﻿Noise emanating from passing ships may distur...      2\n",
       "..                                                 ...    ...\n",
       "373  Intermediate \\nEver since he was diagnosed HIV...      1\n",
       "374  Intermediate \\nGalina Zaglumyonova was woken i...      1\n",
       "375  Intermediate \\nAll six numbers match, so its t...      1\n",
       "376  Intermediate \\nAccording to a recent report, t...      1\n",
       "377  Intermediate \\nWhen Larry Pizzi first heard ab...      1\n",
       "\n",
       "[378 rows x 2 columns]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = adv_df.merge(int_df, how = 'outer')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Swedish prisons have long had a reputation ar...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A lonely old man living in a crater on the moo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿Facebook has lost millions of users per month...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿Noise emanating from passing ships may distur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>﻿Two scientists at Stanford University, in the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>﻿Barack Obama has told young people to reject ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>﻿When two people on a remote Pacific island sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>﻿Google has made maps of the world’s highest m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>﻿Our new international survey, including 33 co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level\n",
       "0    ﻿It was not so much how hard people found the ...      2\n",
       "1    ﻿Swedish prisons have long had a reputation ar...      2\n",
       "2    A lonely old man living in a crater on the moo...      2\n",
       "3    ﻿Facebook has lost millions of users per month...      2\n",
       "4    ﻿Noise emanating from passing ships may distur...      2\n",
       "..                                                 ...    ...\n",
       "562  ﻿Two scientists at Stanford University, in the...      0\n",
       "563  ﻿Barack Obama has told young people to reject ...      0\n",
       "564  ﻿When two people on a remote Pacific island sa...      0\n",
       "565  ﻿Google has made maps of the world’s highest m...      0\n",
       "566  ﻿Our new international survey, including 33 co...      0\n",
       "\n",
       "[567 rows x 2 columns]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df1.merge(ele_df, how = 'outer')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/javm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to downlod punkt to access better tokenization rules\n",
    "# word_tokenize won't work without it\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\\ufeffIt', 'was', 'not', 'so', 'much', 'how', 'hard', 'people', 'found', 'the', 'challenge', 'but', 'how', 'far', 'they', 'would', 'go', 'to', 'avoid', 'it', 'that', 'left', 'researchers', 'gobsmacked', '.', 'The', 'task', '?', 'To', 'sit', 'in', 'a', 'chair', 'and', 'do', 'nothing', 'but', 'think', '.', 'So', 'unbearable', 'did', 'some', 'find', 'it', 'that', 'they', 'took', 'up', 'the', 'safe', 'but', 'alarming', 'opportunity', 'to', 'give', 'themselves', 'mild', 'electric', 'shocks', 'in', 'an', 'attempt', 'to', 'break', 'the', 'tedium', '.', 'Two-thirds', 'of', 'men', 'pressed', 'a', 'button', 'to', 'deliver', 'a', 'painful', 'jolt', 'during', 'a', '15-minute', 'spell', 'of', 'solitude', '.', 'Under', 'the', 'same', 'conditions', ',', 'a', 'quarter', 'of', 'women', 'pressed', 'the', 'shock', 'button', '.', 'The', 'difference', ',', 'scientists', 'suspect', ',', 'is', 'that', 'men', 'tend', 'to', 'be', 'more', 'sensation-seeking', 'than', 'women', '.', 'The', 'report', 'from', 'psychologists', 'at', 'Virginia', 'and', 'Harvard', 'Universities', 'is', 'one', 'of', 'a', 'surprising', 'few', 'to', 'tackle', 'the', 'question', 'of', 'why', 'most', 'of', 'us', 'find', 'it', 'so', 'hard', 'to', 'do', 'nothing', '.'], ['\\ufeffSwedish', 'prisons', 'have', 'long', 'had', 'a', 'reputation', 'around', 'the', 'world', 'for', 'being', 'progressive', '.', 'But', 'are', 'the', 'country', '’', 's', 'prisons', 'a', 'soft', 'option', '?', 'The', 'head', 'of', 'Sweden', '’', 's', 'prison', 'and', 'probation', 'service', ',', 'Nils', 'Oberg', ',', 'announced', 'in', 'November', '2013', 'that', 'four', 'Swedish', 'prisons', 'are', 'to', 'be', 'closed', 'due', 'to', 'an', '“', 'out', 'of', 'the', 'ordinary', '”', 'decline', 'in', 'prisoner', 'numbers', '.', 'Although', 'there', 'has', 'been', 'no', 'fall', 'in', 'crime', 'rates', ',', 'between', '2011', 'and', '2012', 'there', 'was', 'a', '6', '%', 'drop', 'in', 'Sweden', '’', 's', 'prisoner', 'population', ',', 'now', 'a', 'little', 'over', '4,500', '.', 'A', 'similar', 'decrease', 'is', 'expected', 'in', '2013', 'and', '2014', '.', 'Oberg', 'admitted', 'to', 'being', 'puzzled', 'by', 'the', 'unexpected', 'dip', ',', 'but', 'expressed', 'optimism', 'that', 'the', 'reason', 'was', 'to', 'do', 'with', 'how', 'his', 'prisons', 'are', 'run', '.', '“', 'We', 'certainly', 'hope', 'that', 'the', 'efforts', 'we', 'invest', 'in', 'rehabilitation', 'and', 'preventing', 'relapse', 'of', 'crime', 'has', 'had', 'an', 'impact', ',', '”', 'he', 'said', '.', '“', 'The', 'modern', 'prison', 'service', 'in', 'Sweden', 'is', 'very', 'different', 'from', 'when', 'I', 'joined', 'as', 'a', 'young', 'prison', 'officer', 'in', '1978', ',', '”', 'says', 'Kenneth', 'Gustafsson', ',', 'governor', 'of', 'Kumla', 'Prison', ',', 'Sweden', '’', 's', 'most', 'secure', 'jail', ',', 'situated', '130', 'miles', 'west', 'of', 'Stockholm', '.', 'However', ',', 'he', 'doesn', '’', 't', 'think', 'the', 'system', 'has', 'gone', 'soft', '.', '“', 'When', 'I', 'joined', ',', 'the', 'focus', 'was', 'very', 'much', 'on', 'humanity', 'in', 'prisons', '.', 'Prisoners', 'were', 'treated', 'well', '–', 'maybe', 'too', 'well', ',', 'some', 'might', 'say', '.', 'But', ',', 'after', 'a', 'number', 'of', 'high-profile', 'escapes', 'in', '2004', ',', 'we', 'had', 'to', 'rebalance', 'and', 'place', 'more', 'emphasis', 'on', 'security.', '”', 'Despite', 'the', 'hardening', 'of', 'attitudes', 'toward', 'prison', 'security', 'following', 'the', 'escape', 'scandals', ',', 'the', 'Swedes', 'still', 'managed', 'to', 'maintain', 'a', 'broadly', 'humane', 'approach', 'to', 'sentencing', ',', 'even', 'of', 'the', 'most', 'serious', 'offenders', ':', 'jail', 'terms', 'rarely', 'exceed', 'ten', 'years', ';', 'those', 'who', 'receive', 'life', 'imprisonment', 'can', 'still', 'apply', 'to', 'the', 'courts', 'after', 'a', 'decade', 'to', 'have', 'the', 'sentence', 'commuted', 'to', 'a', 'fixed', 'term', ',', 'usually', 'in', 'the', 'region', 'of', '18', 'to', '25', 'years', '.', 'Sweden', 'was', 'the', 'first', 'country', 'in', 'Europe', 'to', 'introduce', 'the', 'electronic', 'tagging', 'of', 'convicted', 'criminals', 'and', 'continues', 'to', 'strive', 'to', 'minimize', 'short-term', 'prison', 'sentences', 'wherever', 'possible', 'by', 'using', 'community-based', 'measures', ',', 'which', 'have', 'been', 'proven', 'to', 'be', 'more', 'effective', 'at', 'reducing', 'reoffending', '.'], ['A', 'lonely', 'old', 'man', 'living', 'in', 'a', 'crater', 'on', 'the', 'moon', 'is', 'the', 'unlikely', 'focus', 'of', 'John', 'Lewis', '’', 's', 'Christmas', '2015', 'advert', ',', 'as', 'the', 'department', 'store', 'puts', 'a', 'charitable', 'spin', 'on', 'its', 'latest', 'multi-million', 'pound', 'campaign', '.', 'Amid', 'increasing', 'hype', 'around', 'John', 'Lewis', '’', 's', 'seasonal', 'ad', ',', 'which', 'has', 'come', 'to', 'mark', 'the', 'beginning', 'of', 'the', 'Christmas', 'shopping', 'season', 'for', 'many', ',', 'the', 'department', 'store', 'will', 'aim', 'to', 'use', 'its', 'profile', 'to', 'raise', 'hundreds', 'of', 'thousands', 'of', 'pounds', 'for', 'Age', 'UK', '.', 'It', 'will', 'also', 'encourage', 'staff', 'and', 'customers', 'to', 'join', 'up', 'with', 'their', 'local', 'branch', 'of', 'the', 'charity', 'to', 'care', 'for', 'elderly', 'people', 'who', 'might', 'otherwise', 'be', 'alone', 'over', 'the', 'holiday', '.', 'The', 'retailer', 'has', 'spent', '£7m', 'on', 'a', 'campaign', 'that', 'ranges', 'from', 'the', 'slick', 'TV', 'ad', 'to', 'a', 'smartphone', 'game', 'and', 'merchandise', ',', 'including', 'glow-in-the-', 'dark', 'pyjamas', ',', 'as', 'well', 'as', 'areas', 'decked', 'out', 'like', 'the', 'surface', 'of', 'the', 'moon', 'in', '11', 'stores', '.', 'After', 'two', 'years', 'of', 'successful', 'ads', 'featuring', 'cuddly', 'animals', '–', 'a', 'bear', 'and', 'hare', ',', 'then', 'a', 'penguin', '–', 'this', 'time', ',', 'the', 'retailer', 'is', 'tugging', 'at', 'the', 'heartstrings', 'with', 'a', 'story', 'of', 'a', 'young', 'girl', ',', 'Lily', ',', 'who', 'spots', 'an', 'old', 'man', 'living', 'in', 'a', 'shack', 'on', 'the', 'moon', 'through', 'her', 'telescope', '.', 'The', 'determined', 'child', 'tries', 'sending', 'him', 'a', 'letter', 'and', 'firing', 'a', 'note', 'via', 'bow', 'and', 'arrow', ',', 'before', 'floating', 'him', 'a', 'present', 'of', 'a', 'telescope', 'tied', 'to', 'balloons', ',', 'which', 'finally', 'enables', 'them', 'to', 'make', 'contact', '.', 'The', 'ad', '’', 's', 'strapline', 'is', ':', '“', 'Show', 'someone', 'they', '’', 're', 'loved', 'this', 'Christmas', '”', ',', 'which', 'echoes', 'Age', 'UK', '’', 's', 'own', 'campaign', ':', '“', 'No', 'one', 'should', 'have', 'no', 'one', 'at', 'Christmas', '”', '.', 'Profits', 'from', 'three', 'products', '–', 'a', 'mug', ',', 'gift', 'tag', 'and', 'card', '–', 'will', 'go', 'to', 'the', 'charity', '.'], ['\\ufeffFacebook', 'has', 'lost', 'millions', 'of', 'users', 'per', 'month', 'in', 'its', 'biggest', 'markets', ',', 'independent', 'data', 'suggests', ',', 'as', 'alternative', 'social', 'networks', 'attract', 'the', 'attention', 'of', 'those', 'looking', 'for', 'fresh', 'online', 'playgrounds', '.', 'As', 'Facebook', 'prepares', 'to', 'update', 'investors', 'on', 'its', 'performance', 'in', 'the', 'first', 'three', 'months', 'of', 'the', 'year', ',', 'with', 'analysts', 'forecasting', 'revenues', 'up', '36', '%', 'on', 'last', 'year', ',', 'studies', 'suggest', 'that', 'its', 'expansion', 'in', 'the', 'US', ',', 'UK', 'and', 'other', 'major', 'European', 'countries', 'has', 'peaked', '.', 'In', 'the', 'last', 'month', ',', 'the', 'world', '’', 's', 'largest', 'social', 'network', 'has', 'lost', '6m', 'US', 'visitors', ',', 'a', '4', '%', 'fall', ',', 'according', 'to', 'analysis', 'firm', 'Socialbakers', '.', 'In', 'the', 'UK', ',', '1.4m', 'fewer', 'users', 'checked', 'in', 'in', 'March', ',', 'a', 'fall', 'of', '4.5', '%', '.', 'The', 'declines', 'are', 'sustained', '.', 'In', 'the', 'last', 'six', 'months', ',', 'Facebook', 'has', 'lost', 'nearly', '9m', 'monthly', 'visitors', 'in', 'the', 'US', 'and', '2m', 'in', 'the', 'UK', '.', 'Users', 'are', 'also', 'switching', 'off', 'in', 'Canada', ',', 'Spain', ',', 'France', ',', 'Germany', 'and', 'Japan', ',', 'where', 'Facebook', 'has', 'some', 'of', 'its', 'biggest', 'followings', '.', 'A', 'spokeswoman', 'for', 'Facebook', 'declined', 'to', 'comment', '.', '“', 'The', 'problem', 'is', 'that', ',', 'in', 'the', 'US', 'and', 'UK', ',', 'most', 'people', 'who', 'want', 'to', 'sign', 'up', 'for', 'Facebook', 'have', 'already', 'done', 'it', ',', '”', 'said', 'new', 'media', 'specialist', 'Ian', 'Maude', 'at', 'Enders', 'Analysis', '.', '“', 'There', 'is', 'a', 'boredom', 'factor', 'where', 'people', 'like', 'to', 'try', 'something', 'new', '.', 'Is', 'Facebook', 'going', 'to', 'go', 'the', 'way', 'of', 'MySpace', '?', 'The', 'risk', 'is', 'relatively', 'small', ',', 'but', 'that', 'is', 'not', 'to', 'say', 'it', 'isn', '’', 't', 'there.', '”', 'Alternative', 'social', 'networks', 'such', 'as', 'Instagram', ',', 'the', 'photo-sharing', 'site', 'that', 'won', '30m', 'users', 'in', '18', 'months', 'before', 'Facebook', 'acquired', 'the', 'business', ',', 'have', 'seen', 'surges', 'in', 'popularity', 'with', 'younger', 'age', 'groups', '.', 'Path', ',', 'the', 'mobile', 'phone-based', 'social', 'network', 'founded', 'by', 'former', 'Facebook', 'employee', 'Dave', 'Morin', ',', 'which', 'restricts', 'its', 'users', 'to', '150', 'friends', ',', 'is', 'gaining', '1m', 'users', 'a', 'week', '.', 'It', 'has', 'recently', 'topped', '9m', 'users', ',', 'with', '500,000', 'Venezuelans', 'downloading', 'the', 'app', 'in', 'a', 'single', 'weekend', '.']]\n"
     ]
    }
   ],
   "source": [
    "corpus = [word_tokenize(doc) for doc in df['Text']]\n",
    "print(corpus[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170398,)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "flattenedcorpus_tokens = pd.Series(list(itertools.chain(*corpus)))\n",
    "print(flattenedcorpus_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12220\n"
     ]
    }
   ],
   "source": [
    "dictionary = pd.Series(flattenedcorpus_tokens.unique())\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",             8422\n",
       "the           8003\n",
       ".             6975\n",
       "of            4310\n",
       "to            3880\n",
       "              ... \n",
       "impress          1\n",
       "positioned       1\n",
       "marking          1\n",
       "Hooft            1\n",
       "papers           1\n",
       "Length: 12220, dtype: int64"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenedcorpus_tokens.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3405"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_one_occurence = (flattenedcorpus_tokens.\n",
    "                     value_counts() < 2).sum()\n",
    "num_one_occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125      2013\n",
       "146      2011\n",
       "147      2012\n",
       "148         6\n",
       "160      2014\n",
       "         ... \n",
       "12064    1989\n",
       "12071    1972\n",
       "12177      41\n",
       "12189      73\n",
       "12201    1623\n",
       "Length: 200, dtype: object"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[dictionary.str.isnumeric()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# imports package with many stopword lists\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get common stop words in english that we'll remove during tokenization/text normalization\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_step_normalizer(doc):\n",
    "    # filters for alphabetic (no punctuation or numbers) and filters out stop words. \n",
    "    # lower cases all tokens\n",
    "    norm_text = [x.lower() for x in word_tokenize(doc) if ((x.isalpha()) & (x.lower() not in stop_words)) ]\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "      <th>tok_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[much, hard, people, found, challenge, far, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Swedish prisons have long had a reputation ar...</td>\n",
       "      <td>2</td>\n",
       "      <td>[prisons, long, reputation, around, world, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A lonely old man living in a crater on the moo...</td>\n",
       "      <td>2</td>\n",
       "      <td>[lonely, old, man, living, crater, moon, unlik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿Facebook has lost millions of users per month...</td>\n",
       "      <td>2</td>\n",
       "      <td>[lost, millions, users, per, month, biggest, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿Noise emanating from passing ships may distur...</td>\n",
       "      <td>2</td>\n",
       "      <td>[emanating, passing, ships, may, disturb, anim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>﻿They may not know who Steve Jobs was or even ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[may, know, steve, jobs, even, tie, shoelaces,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>﻿Writing in the journal Nature, former preside...</td>\n",
       "      <td>2</td>\n",
       "      <td>[journal, nature, former, president, royal, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>﻿The Duke and Duchess of Cambridge won the fir...</td>\n",
       "      <td>2</td>\n",
       "      <td>[duke, duchess, cambridge, first, round, battl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>﻿A Canadian man who sprang to fame after offer...</td>\n",
       "      <td>2</td>\n",
       "      <td>[canadian, man, sprang, fame, offering, free, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>﻿Serial dater Emmanuel Limal was tired of meet...</td>\n",
       "      <td>2</td>\n",
       "      <td>[dater, emmanuel, limal, tired, meeting, women...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Level  \\\n",
       "0  ﻿It was not so much how hard people found the ...      2   \n",
       "1  ﻿Swedish prisons have long had a reputation ar...      2   \n",
       "2  A lonely old man living in a crater on the moo...      2   \n",
       "3  ﻿Facebook has lost millions of users per month...      2   \n",
       "4  ﻿Noise emanating from passing ships may distur...      2   \n",
       "5  ﻿They may not know who Steve Jobs was or even ...      2   \n",
       "6  ﻿Writing in the journal Nature, former preside...      2   \n",
       "7  ﻿The Duke and Duchess of Cambridge won the fir...      2   \n",
       "8  ﻿A Canadian man who sprang to fame after offer...      2   \n",
       "9  ﻿Serial dater Emmanuel Limal was tired of meet...      2   \n",
       "\n",
       "                                            tok_norm  \n",
       "0  [much, hard, people, found, challenge, far, wo...  \n",
       "1  [prisons, long, reputation, around, world, pro...  \n",
       "2  [lonely, old, man, living, crater, moon, unlik...  \n",
       "3  [lost, millions, users, per, month, biggest, m...  \n",
       "4  [emanating, passing, ships, may, disturb, anim...  \n",
       "5  [may, know, steve, jobs, even, tie, shoelaces,...  \n",
       "6  [journal, nature, former, president, royal, in...  \n",
       "7  [duke, duchess, cambridge, first, round, battl...  \n",
       "8  [canadian, man, sprang, fame, offering, free, ...  \n",
       "9  [dater, emmanuel, limal, tired, meeting, women...  "
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tok_norm'] = df['Text'].apply(first_step_normalizer)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9609\n"
     ]
    }
   ],
   "source": [
    "norm_toks_flattened = pd.Series(list(\n",
    "    itertools.chain(*df['tok_norm'])))\n",
    "new_dictionary = norm_toks_flattened.unique()\n",
    "print(len(new_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12220\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removed 5k features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "said           781\n",
       "people         748\n",
       "one            450\n",
       "world          339\n",
       "says           322\n",
       "              ... \n",
       "aisle            1\n",
       "suits            1\n",
       "signing          1\n",
       "legislation      1\n",
       "bricklayer       1\n",
       "Length: 9609, dtype: int64"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_toks_flattened.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/javm/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/javm/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer # lemmatizer using WordNet\n",
    "from nltk.corpus import wordnet # imports WordNet\n",
    "from nltk import pos_tag # nltk's native part of speech tagging\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in untokenized document and returns fully normalized token list\n",
    "def process_doc(doc):\n",
    "\n",
    "    #initialize lemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "        \n",
    "    # remove stop words and punctuations, then lower case\n",
    "    doc_norm = [tok.lower() for tok in word_tokenize(doc) if ((tok.isalpha()) & (tok.lower() not in stop_words)) ]\n",
    "    \n",
    "    #  POS detection on the result will be important in telling Wordnet's lemmatizer how to lemmatize\n",
    "    \n",
    "    # creates list of tuples with tokens and POS tags in wordnet format\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(doc_norm))) \n",
    "    doc_norm = [wnl.lemmatize(token, pos) for token, pos in wordnet_tagged if pos is not None]\n",
    "    \n",
    "    return doc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_normalized_corpus = df['Text'].apply(process_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [much, hard, people, find, challenge, far, go,...\n",
       "1    [prison, long, reputation, world, progressive,...\n",
       "2    [lonely, old, man, living, crater, moon, unlik...\n",
       "3    [lose, million, user, month, big, market, inde...\n",
       "4    [emanate, pass, ship, disturb, animal, killer,...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fully_normalized_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7685"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_fully_norm = pd.Series(list(itertools.chain(*fully_normalized_corpus)))\n",
    "len(flattened_fully_norm.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12220\n"
     ]
    }
   ],
   "source": [
    "## Original dictionary length\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "say           1224\n",
       "people         759\n",
       "make           415\n",
       "year           394\n",
       "world          362\n",
       "              ... \n",
       "frenzy           1\n",
       "emaciated        1\n",
       "gallaghers       1\n",
       "romp             1\n",
       "abstract         1\n",
       "Length: 7685, dtype: int64"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_fully_norm.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #define attributes to store if text preprocessing requires fitting from data\n",
    "        pass\n",
    "    \n",
    "    def fit(self, data, y = 0):\n",
    "        # this is where you would fit things like corpus specific stopwords\n",
    "        # fit probable bigrams with bigram model in here\n",
    "        \n",
    "        # save as parameters of Text preprocessor\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, data, y = 0):\n",
    "        fully_normalized_corpus = data.apply(self.process_doc)\n",
    "        \n",
    "        return fully_normalized_corpus\n",
    "        \n",
    "    \n",
    "    def process_doc(self, doc):\n",
    "\n",
    "        #initialize lemmatizer\n",
    "        wnl = WordNetLemmatizer()\n",
    "        stop_words = stopwords.words('english')\n",
    "        \n",
    "        # helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "        def pos_tagger(nltk_tag):\n",
    "            if nltk_tag.startswith('J'):\n",
    "                return wordnet.ADJ\n",
    "            elif nltk_tag.startswith('V'):\n",
    "                return wordnet.VERB\n",
    "            elif nltk_tag.startswith('N'):\n",
    "                return wordnet.NOUN\n",
    "            elif nltk_tag.startswith('R'):\n",
    "                return wordnet.ADV\n",
    "            else:         \n",
    "                return None\n",
    "\n",
    "\n",
    "        # remove stop words and punctuations, then lower case\n",
    "        doc_norm = [tok.lower() for tok in word_tokenize(doc) if ((tok.isalpha()) & (tok not in stop_words)) ]\n",
    "\n",
    "        #  POS detection on the result will be important in telling Wordnet's lemmatizer how to lemmatize\n",
    "\n",
    "        # creates list of tuples with tokens and POS tags in wordnet format\n",
    "        wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(doc_norm))) \n",
    "        doc_norm = [wnl.lemmatize(token, pos) for token, pos in wordnet_tagged if pos is not None]\n",
    "\n",
    "        return \" \".join(doc_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Text']\n",
    "y = df['Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "proc = TextPreprocessor()\n",
    "\n",
    "# again this kind of splitting only becomes important if fitting of text transformers fits to statistics of the text corpus\n",
    "transformed_train = proc.fit_transform(X_train)\n",
    "transformed_test = proc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_steps = [('countvec', CountVectorizer(min_df =.03, max_df = .5))]\n",
    "preprocess_pipeline = Pipeline(prc_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_proc = preprocess_pipeline.fit_transform(transformed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396, 715)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_proc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_proc = preprocess_pipeline.transform(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "      <th>tok_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[much, hard, people, found, challenge, far, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿Swedish prisons have long had a reputation ar...</td>\n",
       "      <td>2</td>\n",
       "      <td>[prisons, long, reputation, around, world, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A lonely old man living in a crater on the moo...</td>\n",
       "      <td>2</td>\n",
       "      <td>[lonely, old, man, living, crater, moon, unlik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>﻿Facebook has lost millions of users per month...</td>\n",
       "      <td>2</td>\n",
       "      <td>[lost, millions, users, per, month, biggest, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>﻿Noise emanating from passing ships may distur...</td>\n",
       "      <td>2</td>\n",
       "      <td>[emanating, passing, ships, may, disturb, anim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>﻿Two scientists at Stanford University, in the...</td>\n",
       "      <td>0</td>\n",
       "      <td>[scientists, stanford, university, usa, used, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>﻿Barack Obama has told young people to reject ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[obama, told, young, people, reject, pessimism...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>﻿When two people on a remote Pacific island sa...</td>\n",
       "      <td>0</td>\n",
       "      <td>[two, people, remote, pacific, island, saw, sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>﻿Google has made maps of the world’s highest m...</td>\n",
       "      <td>0</td>\n",
       "      <td>[made, maps, world, highest, mountains, ocean,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>﻿Our new international survey, including 33 co...</td>\n",
       "      <td>0</td>\n",
       "      <td>[new, international, survey, including, countr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level  \\\n",
       "0    ﻿It was not so much how hard people found the ...      2   \n",
       "1    ﻿Swedish prisons have long had a reputation ar...      2   \n",
       "2    A lonely old man living in a crater on the moo...      2   \n",
       "3    ﻿Facebook has lost millions of users per month...      2   \n",
       "4    ﻿Noise emanating from passing ships may distur...      2   \n",
       "..                                                 ...    ...   \n",
       "562  ﻿Two scientists at Stanford University, in the...      0   \n",
       "563  ﻿Barack Obama has told young people to reject ...      0   \n",
       "564  ﻿When two people on a remote Pacific island sa...      0   \n",
       "565  ﻿Google has made maps of the world’s highest m...      0   \n",
       "566  ﻿Our new international survey, including 33 co...      0   \n",
       "\n",
       "                                              tok_norm  \n",
       "0    [much, hard, people, found, challenge, far, wo...  \n",
       "1    [prisons, long, reputation, around, world, pro...  \n",
       "2    [lonely, old, man, living, crater, moon, unlik...  \n",
       "3    [lost, millions, users, per, month, biggest, m...  \n",
       "4    [emanating, passing, ships, may, disturb, anim...  \n",
       "..                                                 ...  \n",
       "562  [scientists, stanford, university, usa, used, ...  \n",
       "563  [obama, told, young, people, reject, pessimism...  \n",
       "564  [two, people, remote, pacific, island, saw, sm...  \n",
       "565  [made, maps, world, highest, mountains, ocean,...  \n",
       "566  [new, international, survey, including, countr...  \n",
       "\n",
       "[567 rows x 3 columns]"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>access</th>\n",
       "      <th>accord</th>\n",
       "      <th>action</th>\n",
       "      <th>add</th>\n",
       "      <th>admit</th>\n",
       "      <th>adult</th>\n",
       "      <th>affect</th>\n",
       "      <th>africa</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>worker</th>\n",
       "      <th>world</th>\n",
       "      <th>worry</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>396 rows × 715 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ability  able  access  accord  action  add  admit  adult  affect  africa  \\\n",
       "0          0     0       0       0       1    0      0      0       0       0   \n",
       "1          0     0       0       0       0    0      0      0       0       0   \n",
       "2          1     0       0       0       0    0      0      0       0       0   \n",
       "3          0     0       0       0       0    0      0      0       0       0   \n",
       "4          0     0       0       0       0    0      0      0       0       0   \n",
       "..       ...   ...     ...     ...     ...  ...    ...    ...     ...     ...   \n",
       "391        0     0       0       0       0    0      2      0       2       0   \n",
       "392        0     0       0       0       0    0      0      0       0       1   \n",
       "393        0     0       0       0       0    1      0      0       0       0   \n",
       "394        0     0       0       0       0    0      0      0       0       0   \n",
       "395        0     0       0       0       0    0      0      0       0       0   \n",
       "\n",
       "     ...  work  worker  world  worry  write  wrong  year  yet  york  young  \n",
       "0    ...     1       0      0      0      0      0     0    0     0      0  \n",
       "1    ...     0       0      0      0      0      0     1    0     0      0  \n",
       "2    ...     0       0      3      0      0      0     0    0     0      0  \n",
       "3    ...     3       0      0      0      0      0     0    0     0      0  \n",
       "4    ...     4       0      0      0      1      0     1    0     0      0  \n",
       "..   ...   ...     ...    ...    ...    ...    ...   ...  ...   ...    ...  \n",
       "391  ...     0       0      0      0      1      0     0    0     0      0  \n",
       "392  ...     0       0      0      0      0      0     0    0     0      0  \n",
       "393  ...     3       0      0      0      0      0     0    0     0      0  \n",
       "394  ...     0       0      0      1      0      0     0    0     1      0  \n",
       "395  ...     0       0      1      0      0      0     0    0     0      0  \n",
       "\n",
       "[396 rows x 715 columns]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_names = preprocess_pipeline[\n",
    "    'countvec'].get_feature_names()\n",
    "\n",
    "pd.DataFrame(X_tr_proc.toarray(), columns = feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>access</th>\n",
       "      <th>accord</th>\n",
       "      <th>action</th>\n",
       "      <th>add</th>\n",
       "      <th>admit</th>\n",
       "      <th>adult</th>\n",
       "      <th>affect</th>\n",
       "      <th>africa</th>\n",
       "      <th>...</th>\n",
       "      <th>worker</th>\n",
       "      <th>world</th>\n",
       "      <th>worry</th>\n",
       "      <th>write</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>396 rows × 716 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ability  able  access  accord  action  add  admit  adult  affect  africa  \\\n",
       "0          0     0       0       0       1    0      0      0       0       0   \n",
       "1          0     0       0       0       0    0      0      0       0       0   \n",
       "2          1     0       0       0       0    0      0      0       0       0   \n",
       "3          0     0       0       0       0    0      0      0       0       0   \n",
       "4          0     0       0       0       0    0      0      0       0       0   \n",
       "..       ...   ...     ...     ...     ...  ...    ...    ...     ...     ...   \n",
       "391        0     0       0       0       0    0      2      0       2       0   \n",
       "392        0     0       0       0       0    0      0      0       0       1   \n",
       "393        0     0       0       0       0    1      0      0       0       0   \n",
       "394        0     0       0       0       0    0      0      0       0       0   \n",
       "395        0     0       0       0       0    0      0      0       0       0   \n",
       "\n",
       "     ...  worker  world  worry  write  wrong  year  yet  york  young  Level  \n",
       "0    ...       0      0      0      0      0     0    0     0      0    2.0  \n",
       "1    ...       0      0      0      0      0     1    0     0      0    2.0  \n",
       "2    ...       0      3      0      0      0     0    0     0      0    2.0  \n",
       "3    ...       0      0      0      0      0     0    0     0      0    2.0  \n",
       "4    ...       0      0      0      1      0     1    0     0      0    2.0  \n",
       "..   ...     ...    ...    ...    ...    ...   ...  ...   ...    ...    ...  \n",
       "391  ...       0      0      0      1      0     0    0     0      0    0.0  \n",
       "392  ...       0      0      0      0      0     0    0     0      0    0.0  \n",
       "393  ...       0      0      0      0      0     0    0     0      0    NaN  \n",
       "394  ...       0      0      1      0      0     0    0     1      0    0.0  \n",
       "395  ...       0      1      0      0      0     0    0     0      0    0.0  \n",
       "\n",
       "[396 rows x 716 columns]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_mat = pd.DataFrame(X_tr_proc.toarray(), columns = feat_names)\n",
    "bow_mat['Level'] = y_train\n",
    "bow_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make     0.011394\n",
       "world    0.011179\n",
       "year     0.010319\n",
       "take     0.008707\n",
       "food     0.007524\n",
       "use      0.007417\n",
       "go       0.007094\n",
       "want     0.007094\n",
       "many     0.006665\n",
       "time     0.006557\n",
       "dtype: float64"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2_bow_mat = bow_mat[bow_mat['Level'] == 2].drop(columns = ['Level'])\n",
    "\n",
    "# class 1 token probabilities:\n",
    "N_tok_2 = class2_bow_mat.sum(axis = 0) \n",
    "N_2 =  class2_bow_mat.values.sum()\n",
    "\n",
    "# get probabilities for each token: class 1\n",
    "proba_c2 = N_tok_2/N_2\n",
    "\n",
    "proba_c2.sort_values(ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make     0.012136\n",
       "year     0.010900\n",
       "world    0.009664\n",
       "new      0.008877\n",
       "use      0.008540\n",
       "get      0.008203\n",
       "work     0.007866\n",
       "find     0.007304\n",
       "also     0.007304\n",
       "big      0.007192\n",
       "dtype: float64"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class1_bow_mat = bow_mat[bow_mat['Level'] == 1].drop(columns = ['Level'])\n",
    "\n",
    "# class 1 token probabilities:\n",
    "N_tok_1 = class1_bow_mat.sum(axis = 0) \n",
    "N_1 =  class1_bow_mat.values.sum()\n",
    "\n",
    "# get probabilities for each token: class 1\n",
    "proba_c1 = N_tok_1/N_1\n",
    "\n",
    "proba_c1.sort_values(ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "use           0.020346\n",
       "want          0.015259\n",
       "new           0.013225\n",
       "factory       0.012208\n",
       "world         0.010173\n",
       "month         0.009156\n",
       "charity       0.009156\n",
       "make          0.009156\n",
       "restaurant    0.008138\n",
       "take          0.008138\n",
       "dtype: float64"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class0_bow_mat = bow_mat[bow_mat['Level'] == 0].drop(columns = ['Level'])\n",
    "\n",
    "# class 1 token probabilities:\n",
    "N_tok_0 = class0_bow_mat.sum(axis = 0)\n",
    "N_0 =  class0_bow_mat.values.sum() \n",
    "\n",
    "# get probabilities for each token: class 0\n",
    "proba_c0 = N_tok_0/N_0\n",
    "\n",
    "proba_c0.sort_values(ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Multinomial Naive Bayes Classifier to pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countvec', CountVectorizer(max_df=0.5, min_df=0.03)),\n",
       " ('multinb', MultinomialNB())]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "mod_pipe = deepcopy(preprocess_pipeline)\n",
    "mod_pipe.steps.append(('multinb', MultinomialNB()))\n",
    "mod_pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvec', CountVectorizer(max_df=0.5, min_df=0.03)),\n",
       "                ('multinb', MultinomialNB())])"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_pipe.fit(transformed_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test = proc.transform(X_test)\n",
    "\n",
    "y_pred = mod_pipe.predict(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.48      0.51        54\n",
      "           1       0.42      0.53      0.47        59\n",
      "           2       0.41      0.36      0.39        58\n",
      "\n",
      "    accuracy                           0.46       171\n",
      "   macro avg       0.46      0.46      0.46       171\n",
      "weighted avg       0.46      0.46      0.46       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(cv=5)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegressionCV(fit_intercept=True, cv= 5, \n",
    "                     dual=False, penalty='l2', scoring=None, \n",
    "                     solver='lbfgs', tol=0.0001, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    1.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'gamma': 1, 'kernel': 'linear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56        54\n",
      "           1       1.00      0.97      0.98        59\n",
      "           2       0.59      0.66      0.62        58\n",
      "\n",
      "    accuracy                           0.73       171\n",
      "   macro avg       0.72      0.72      0.72       171\n",
      "weighted avg       0.73      0.73      0.73       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    2.4s finished\n"
     ]
    }
   ],
   "source": [
    "# defining parameter range \n",
    "param_grid = {'C': [0.1, 1, 10, 100],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              #'gamma':['scale', 'auto'],\n",
    "              'kernel': ['linear']}  \n",
    "   \n",
    "grid = GridSearchCV(SVC(random_state = 42), param_grid, refit = True, verbose = 3,n_jobs=-1) \n",
    "   \n",
    "# fitting the model for grid search \n",
    "grid.fit(X_tr_proc, y_train) \n",
    " \n",
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "grid_predictions = grid.predict(X_test_proc) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(y_test, grid_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFID Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_steps2 = [('tfid', TfidfVectorizer(min_df = 0.05, max_df = 0.95))]\n",
    "preprocess_pipeline2 = Pipeline(prc_steps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_proc2 = preprocess_pipeline2.fit_transform(transformed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_proc2 = preprocess_pipeline2.transform(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100, 'gamma': 1, 'kernel': 'linear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.65      0.60        54\n",
      "           1       0.89      0.83      0.86        59\n",
      "           2       0.59      0.55      0.57        58\n",
      "\n",
      "    accuracy                           0.68       171\n",
      "   macro avg       0.68      0.68      0.68       171\n",
      "weighted avg       0.69      0.68      0.68       171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "# defining parameter range \n",
    "param_grid = {'C': [0.1, 1, 10, 100],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              #'gamma':['scale', 'auto'],\n",
    "              'kernel': ['linear']}  \n",
    "   \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3,n_jobs=-1) \n",
    "   \n",
    "# fitting the model for grid search \n",
    "grid.fit(X_tr_proc2, y_train) \n",
    " \n",
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "grid_predictions2 = grid.predict(X_test_proc2) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(y_test, grid_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.48      0.51        54\n",
      "           1       0.42      0.53      0.47        59\n",
      "           2       0.41      0.36      0.39        58\n",
      "\n",
      "    accuracy                           0.46       171\n",
      "   macro avg       0.46      0.46      0.46       171\n",
      "weighted avg       0.46      0.46      0.46       171\n",
      "\n",
      "[[26 13 15]\n",
      " [13 31 15]\n",
      " [ 8 29 21]]\n",
      "Accuracy of the model on Testing Sample Data: 0.46\n",
      "\n",
      "Accuracy values for 5-fold Cross Validation:\n",
      " [nan nan nan nan nan]\n",
      "\n",
      "Final Average Accuracy of the model: nan\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    " \n",
    "# GaussianNB is used in Binomial Classification\n",
    "# MultinomialNB is used in multi-class classification\n",
    "#clf = GaussianNB()\n",
    "clf = MultinomialNB()\n",
    " \n",
    "# Printing all the parameters of Naive Bayes\n",
    "print(clf)\n",
    " \n",
    "NB=clf.fit(X_tr_proc,y_train)\n",
    "prediction=NB.predict(X_test_proc)\n",
    " \n",
    "# Measuring accuracy on Testing Data\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(metrics.confusion_matrix(y_test, prediction))\n",
    " \n",
    "# Printing the Overall Accuracy of the model\n",
    "F1_Score=metrics.f1_score(y_test, prediction, average='weighted')\n",
    "print('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n",
    " \n",
    "# Importing cross validation function from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "# Running 10-Fold Cross validation on a given algorithm\n",
    "# Passing full data X and y because the K-fold will split the data and automatically choose train/test\n",
    "Accuracy_Values=cross_val_score(NB, X , y, cv=5, scoring='f1_weighted')\n",
    "print('\\nAccuracy values for 5-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplementNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.54      0.45        54\n",
      "           1       0.50      0.39      0.44        59\n",
      "           2       0.35      0.29      0.32        58\n",
      "\n",
      "    accuracy                           0.40       171\n",
      "   macro avg       0.41      0.41      0.40       171\n",
      "weighted avg       0.41      0.40      0.40       171\n",
      "\n",
      "[[29  6 19]\n",
      " [23 23 13]\n",
      " [24 17 17]]\n",
      "Accuracy of the model on Testing Sample Data: 0.4\n",
      "\n",
      "Accuracy values for 5-fold Cross Validation:\n",
      " [nan nan nan nan nan]\n",
      "\n",
      "Final Average Accuracy of the model: nan\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\n",
    " \n",
    "# GaussianNB is used in Binomial Classification\n",
    "# MultinomialNB is used in multi-class classification\n",
    "#clf = GaussianNB()\n",
    "clf = ComplementNB()\n",
    " \n",
    "# Printing all the parameters of Naive Bayes\n",
    "print(clf)\n",
    " \n",
    "NB=clf.fit(X_tr_proc2,y_train)\n",
    "prediction=NB.predict(X_test_proc2)\n",
    " \n",
    "# Measuring accuracy on Testing Data\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(metrics.confusion_matrix(y_test, prediction))\n",
    " \n",
    "# Printing the Overall Accuracy of the model\n",
    "F1_Score=metrics.f1_score(y_test, prediction, average='weighted')\n",
    "print('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n",
    " \n",
    "# Importing cross validation function from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "# Running 10-Fold Cross validation on a given algorithm\n",
    "# Passing full data X and y because the K-fold will split the data and automatically choose train/test\n",
    "Accuracy_Values=cross_val_score(NB, X , y, cv=5, scoring='f1_weighted')\n",
    "print('\\nAccuracy values for 5-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
