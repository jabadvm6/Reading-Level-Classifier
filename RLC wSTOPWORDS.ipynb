{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Level Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/javm/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\ufeffIt was not so much how hard people found the challenge but how far they would go to avoid it that left researchers gobsmacked. The task? To sit in a chair and do nothing but think. \\n',\n",
       "  'So unbearable did some find it that they took up the safe but alarming opportunity to give themselves mild electric shocks in an attempt to break the tedium. \\n',\n",
       "  'Two-thirds of men pressed a button to deliver a painful jolt during a 15-minute spell of solitude. \\n',\n",
       "  'Under the same conditions, a quarter of women pressed the shock button. The difference, scientists suspect, is that men tend to be more sensation-seeking than women. \\n',\n",
       "  'The report from psychologists at Virginia and Harvard Universities is one of a surprising few to tackle the question of why most of us find it so hard to do nothing.'],\n",
       " ['\\ufeffSwedish prisons have long had a reputation around the world for being progressive. But are the country’s prisons a soft option? \\n',\n",
       "  'The head of Sweden’s prison and probation service, Nils Oberg, announced in November 2013 that four Swedish prisons are to be closed due to an “out of the ordinary” decline in prisoner numbers. \\n',\n",
       "  'Although there has been no fall in crime rates, between 2011 and 2012 there was a 6% drop in Sweden’s prisoner population, now a little over 4,500. A similar decrease is expected in 2013 and 2014. Oberg admitted to being puzzled by the unexpected dip, but expressed optimism that the reason was to do with how his prisons are run. “We certainly hope that the efforts we invest in rehabilitation and preventing relapse of crime has had an impact,” he said. \\n',\n",
       "  '“The modern prison service in Sweden is very different from when I joined as a young prison officer in 1978,” says Kenneth Gustafsson, governor of Kumla Prison, Sweden’s most secure jail, situated 130 miles west of Stockholm. However, he doesn’t think the system has gone soft. “When I joined, the focus was very much on humanity in prisons. Prisoners were treated well – maybe too well, some might say. But, after a number of high-profile escapes in 2004, we had to rebalance and place more emphasis on security.” \\n',\n",
       "  'Despite the hardening of attitudes toward prison security following the escape scandals, the Swedes still managed to maintain a broadly humane approach to sentencing, even of the most serious offenders: jail terms rarely exceed ten years; those who receive life imprisonment can still apply to the courts after a decade to have the sentence commuted to a fixed term, usually in the region of 18 to 25 years. Sweden was the first country in Europe to introduce the electronic tagging of convicted criminals and continues to strive to minimize short-term prison sentences wherever possible by using community-based measures, which have been proven to be more effective at reducing reoffending.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_list = []\n",
    "advanced = '/Users/javm/Desktop/Flatiron4/Reading-Level-Classifier/Texts-SeparatedByReadingLevel/Adv-Txt'\n",
    "for i in (os.listdir(advanced)):\n",
    "    with open(advanced+'/'+i) as f:\n",
    "        lines = f.readlines()\n",
    "        adv_list.append(lines)\n",
    "        \n",
    "adv_list[0:2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_list = flatten(adv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So unbearable did some find it that they took ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two-thirds of men pressed a button to deliver ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Under the same conditions, a quarter of women ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The report from psychologists at Virginia and ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>﻿Tigers are more numerous in Nepal than at any...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>The number of wild royal bengal tigers in Nepa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>The census is based on the examination of pict...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>Dhakal said that a parallel survey was conduct...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>Nepal has pledged to double the population of ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>945 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level\n",
       "0    ﻿It was not so much how hard people found the ...      2\n",
       "1    So unbearable did some find it that they took ...      2\n",
       "2    Two-thirds of men pressed a button to deliver ...      2\n",
       "3    Under the same conditions, a quarter of women ...      2\n",
       "4    The report from psychologists at Virginia and ...      2\n",
       "..                                                 ...    ...\n",
       "940  ﻿Tigers are more numerous in Nepal than at any...      2\n",
       "941  The number of wild royal bengal tigers in Nepa...      2\n",
       "942  The census is based on the examination of pict...      2\n",
       "943  Dhakal said that a parallel survey was conduct...      2\n",
       "944  Nepal has pledged to double the population of ...      2\n",
       "\n",
       "[945 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize list of lists\n",
    "data1 = adv_list\n",
    "# Create the pandas DataFrame\n",
    "adv_df = pd.DataFrame(data1, columns = ['Text'] )\n",
    "# print dataframe.\n",
    "adv_df['Level'] = 2\n",
    "adv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_list = []\n",
    "intermediate = '/Users/javm/Desktop/Flatiron4/Reading-Level-Classifier/Texts-SeparatedByReadingLevel/Int-Txt'\n",
    "for i in (os.listdir(intermediate)):\n",
    "    with open(intermediate+'/'+i) as f:\n",
    "        lines = f.readlines()\n",
    "        int_list.append(lines)\n",
    "        \n",
    "#print(int_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_list = flatten(int_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intermediate \\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Valdevaqueros is one of the last unspoilt beac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For years it has been very different from the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Traditional jobs such as fishing are dying out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Opponents of the complex say more housing is n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>Intermediate \\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>When Larry Pizzi first heard about electric bi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>Its a question that still puzzles some people....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>Pizzi, who is now CEO of Currie Technologies, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>The US is different from other countries when ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>945 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level\n",
       "0                                      Intermediate \\n      1\n",
       "1    Valdevaqueros is one of the last unspoilt beac...      1\n",
       "2    For years it has been very different from the ...      1\n",
       "3    Traditional jobs such as fishing are dying out...      1\n",
       "4    Opponents of the complex say more housing is n...      1\n",
       "..                                                 ...    ...\n",
       "940                                    Intermediate \\n      1\n",
       "941  When Larry Pizzi first heard about electric bi...      1\n",
       "942  Its a question that still puzzles some people....      1\n",
       "943  Pizzi, who is now CEO of Currie Technologies, ...      1\n",
       "944  The US is different from other countries when ...      1\n",
       "\n",
       "[945 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize list of lists\n",
    "data3 = int_list\n",
    "# Create the pandas DataFrame\n",
    "int_df = pd.DataFrame(data3, columns = ['Text'] )\n",
    "# print dataframe.\n",
    "int_df['Level'] = 1\n",
    "int_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_list = []\n",
    "ele = '/Users/javm/Desktop/Flatiron4/Reading-Level-Classifier/Texts-SeparatedByReadingLevel/Ele-Txt'\n",
    "for i in (os.listdir(ele)):\n",
    "    with open(ele+'/'+i) as f:\n",
    "        lines = f.readlines()\n",
    "        ele_list.append(lines)\n",
    "        \n",
    "#print(ele_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele_list = flatten(ele_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿SeaWorld’s profits fell by 84% and customers ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The company teaches dolphins and killer whales...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It says fewer people are going to its parks an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SeaWorld has been in the news since the 2013 d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Animal rights organizations say that orcas kep...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>﻿Our new international survey, including 33 co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>British people think the richest 1% own 59% of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>Brazilians think the average age in their coun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>In Britain, people think that 43% of young adu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>So, why do people across the world know so lit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>945 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  Level\n",
       "0    ﻿SeaWorld’s profits fell by 84% and customers ...      0\n",
       "1    The company teaches dolphins and killer whales...      0\n",
       "2    It says fewer people are going to its parks an...      0\n",
       "3    SeaWorld has been in the news since the 2013 d...      0\n",
       "4    Animal rights organizations say that orcas kep...      0\n",
       "..                                                 ...    ...\n",
       "940  ﻿Our new international survey, including 33 co...      0\n",
       "941  British people think the richest 1% own 59% of...      0\n",
       "942  Brazilians think the average age in their coun...      0\n",
       "943  In Britain, people think that 43% of young adu...      0\n",
       "944  So, why do people across the world know so lit...      0\n",
       "\n",
       "[945 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize list of lists\n",
    "data2 = ele_list\n",
    "# Create the pandas DataFrame\n",
    "ele_df = pd.DataFrame(data2, columns = ['Text'] )\n",
    "# print dataframe.\n",
    "ele_df['Level'] = 0\n",
    "ele_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So unbearable did some find it that they took ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two-thirds of men pressed a button to deliver ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Under the same conditions, a quarter of women ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The report from psychologists at Virginia and ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>A booming luxury market is serving the new ric...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>When Larry Pizzi first heard about electric bi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>Its a question that still puzzles some people....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>Pizzi, who is now CEO of Currie Technologies, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>The US is different from other countries when ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1890 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Level\n",
       "0     ﻿It was not so much how hard people found the ...      2\n",
       "1     So unbearable did some find it that they took ...      2\n",
       "2     Two-thirds of men pressed a button to deliver ...      2\n",
       "3     Under the same conditions, a quarter of women ...      2\n",
       "4     The report from psychologists at Virginia and ...      2\n",
       "...                                                 ...    ...\n",
       "1885  A booming luxury market is serving the new ric...      1\n",
       "1886  When Larry Pizzi first heard about electric bi...      1\n",
       "1887  Its a question that still puzzles some people....      1\n",
       "1888  Pizzi, who is now CEO of Currie Technologies, ...      1\n",
       "1889  The US is different from other countries when ...      1\n",
       "\n",
       "[1890 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = adv_df.merge(int_df, how = 'outer')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So unbearable did some find it that they took ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two-thirds of men pressed a button to deliver ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Under the same conditions, a quarter of women ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The report from psychologists at Virginia and ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>﻿Our new international survey, including 33 co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>British people think the richest 1% own 59% of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>Brazilians think the average age in their coun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>In Britain, people think that 43% of young adu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>So, why do people across the world know so lit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2835 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Level\n",
       "0     ﻿It was not so much how hard people found the ...      2\n",
       "1     So unbearable did some find it that they took ...      2\n",
       "2     Two-thirds of men pressed a button to deliver ...      2\n",
       "3     Under the same conditions, a quarter of women ...      2\n",
       "4     The report from psychologists at Virginia and ...      2\n",
       "...                                                 ...    ...\n",
       "2830  ﻿Our new international survey, including 33 co...      0\n",
       "2831  British people think the richest 1% own 59% of...      0\n",
       "2832  Brazilians think the average age in their coun...      0\n",
       "2833  In Britain, people think that 43% of young adu...      0\n",
       "2834  So, why do people across the world know so lit...      0\n",
       "\n",
       "[2835 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df1.merge(ele_df, how = 'outer')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/javm/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to downlod punkt to access better tokenization rules\n",
    "# word_tokenize won't work without it\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\\ufeffIt', 'was', 'not', 'so', 'much', 'how', 'hard', 'people', 'found', 'the', 'challenge', 'but', 'how', 'far', 'they', 'would', 'go', 'to', 'avoid', 'it', 'that', 'left', 'researchers', 'gobsmacked', '.', 'The', 'task', '?', 'To', 'sit', 'in', 'a', 'chair', 'and', 'do', 'nothing', 'but', 'think', '.'], ['So', 'unbearable', 'did', 'some', 'find', 'it', 'that', 'they', 'took', 'up', 'the', 'safe', 'but', 'alarming', 'opportunity', 'to', 'give', 'themselves', 'mild', 'electric', 'shocks', 'in', 'an', 'attempt', 'to', 'break', 'the', 'tedium', '.'], ['Two-thirds', 'of', 'men', 'pressed', 'a', 'button', 'to', 'deliver', 'a', 'painful', 'jolt', 'during', 'a', '15-minute', 'spell', 'of', 'solitude', '.'], ['Under', 'the', 'same', 'conditions', ',', 'a', 'quarter', 'of', 'women', 'pressed', 'the', 'shock', 'button', '.', 'The', 'difference', ',', 'scientists', 'suspect', ',', 'is', 'that', 'men', 'tend', 'to', 'be', 'more', 'sensation-seeking', 'than', 'women', '.']]\n"
     ]
    }
   ],
   "source": [
    "corpus = [word_tokenize(doc) for doc in df['Text']]\n",
    "print(corpus[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170594,)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "flattenedcorpus_tokens = pd.Series(list(itertools.chain(*corpus)))\n",
    "print(flattenedcorpus_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12108\n"
     ]
    }
   ],
   "source": [
    "dictionary = pd.Series(flattenedcorpus_tokens.unique())\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",               8422\n",
       "the             8003\n",
       ".               7171\n",
       "of              4310\n",
       "to              3880\n",
       "                ... \n",
       "90-day             1\n",
       "odd                1\n",
       "forgetting         1\n",
       "snapped            1\n",
       "observations       1\n",
       "Length: 12108, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenedcorpus_tokens.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3330"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_one_occurence = (flattenedcorpus_tokens.\n",
    "                     value_counts() < 2).sum()\n",
    "num_one_occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125      2013\n",
       "146      2011\n",
       "147      2012\n",
       "148         6\n",
       "160      2014\n",
       "         ... \n",
       "11959    1989\n",
       "11966    1972\n",
       "12067      41\n",
       "12078      73\n",
       "12090    1623\n",
       "Length: 200, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[dictionary.str.isnumeric()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# imports package with many stopword lists\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get common stop words in english that we'll remove during tokenization/text normalization\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_step_normalizer(doc):\n",
    "    # filters for alphabetic (no punctuation or numbers) and filters out stop words. \n",
    "    # lower cases all tokens\n",
    "    norm_text = [x.lower() for x in word_tokenize(doc) if ((x.isalpha()) & (x.lower() not in stop_words)) ]\n",
    "    return norm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Level</th>\n",
       "      <th>tok_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[much, hard, people, found, challenge, far, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So unbearable did some find it that they took ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[unbearable, find, took, safe, alarming, oppor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two-thirds of men pressed a button to deliver ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[men, pressed, button, deliver, painful, jolt,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Under the same conditions, a quarter of women ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[conditions, quarter, women, pressed, shock, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The report from psychologists at Virginia and ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[report, psychologists, virginia, harvard, uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>﻿Swedish prisons have long had a reputation ar...</td>\n",
       "      <td>2</td>\n",
       "      <td>[prisons, long, reputation, around, world, pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The head of Sweden’s prison and probation serv...</td>\n",
       "      <td>2</td>\n",
       "      <td>[head, sweden, prison, probation, service, nil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Although there has been no fall in crime rates...</td>\n",
       "      <td>2</td>\n",
       "      <td>[although, fall, crime, rates, drop, sweden, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>“The modern prison service in Sweden is very d...</td>\n",
       "      <td>2</td>\n",
       "      <td>[modern, prison, service, sweden, different, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Despite the hardening of attitudes toward pris...</td>\n",
       "      <td>2</td>\n",
       "      <td>[despite, hardening, attitudes, toward, prison...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Level  \\\n",
       "0  ﻿It was not so much how hard people found the ...      2   \n",
       "1  So unbearable did some find it that they took ...      2   \n",
       "2  Two-thirds of men pressed a button to deliver ...      2   \n",
       "3  Under the same conditions, a quarter of women ...      2   \n",
       "4  The report from psychologists at Virginia and ...      2   \n",
       "5  ﻿Swedish prisons have long had a reputation ar...      2   \n",
       "6  The head of Sweden’s prison and probation serv...      2   \n",
       "7  Although there has been no fall in crime rates...      2   \n",
       "8  “The modern prison service in Sweden is very d...      2   \n",
       "9  Despite the hardening of attitudes toward pris...      2   \n",
       "\n",
       "                                            tok_norm  \n",
       "0  [much, hard, people, found, challenge, far, wo...  \n",
       "1  [unbearable, find, took, safe, alarming, oppor...  \n",
       "2  [men, pressed, button, deliver, painful, jolt,...  \n",
       "3  [conditions, quarter, women, pressed, shock, b...  \n",
       "4  [report, psychologists, virginia, harvard, uni...  \n",
       "5  [prisons, long, reputation, around, world, pro...  \n",
       "6  [head, sweden, prison, probation, service, nil...  \n",
       "7  [although, fall, crime, rates, drop, sweden, p...  \n",
       "8  [modern, prison, service, sweden, different, j...  \n",
       "9  [despite, hardening, attitudes, toward, prison...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tok_norm'] = df['Text'].apply(first_step_normalizer)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9613\n"
     ]
    }
   ],
   "source": [
    "norm_toks_flattened = pd.Series(list(\n",
    "    itertools.chain(*df['tok_norm'])))\n",
    "new_dictionary = norm_toks_flattened.unique()\n",
    "print(len(new_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12108\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removed 5k features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "said              781\n",
       "people            752\n",
       "one               452\n",
       "world             339\n",
       "says              322\n",
       "                 ... \n",
       "municipalities      1\n",
       "easierfor           1\n",
       "kickbacks           1\n",
       "legislation         1\n",
       "noting              1\n",
       "Length: 9613, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_toks_flattened.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/javm/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/javm/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer # lemmatizer using WordNet\n",
    "from nltk.corpus import wordnet # imports WordNet\n",
    "from nltk import pos_tag # nltk's native part of speech tagging\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in untokenized document and returns fully normalized token list\n",
    "def process_doc(doc):\n",
    "\n",
    "    #initialize lemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "        \n",
    "    # remove stop words and punctuations, then lower case\n",
    "    doc_norm = [tok.lower() for tok in word_tokenize(doc) if (tok.isalpha())]\n",
    "    \n",
    "    #  POS detection on the result will be important in telling Wordnet's lemmatizer how to lemmatize\n",
    "    \n",
    "    # creates list of tuples with tokens and POS tags in wordnet format\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(doc_norm))) \n",
    "    doc_norm = [wnl.lemmatize(token, pos) for token, pos in wordnet_tagged if pos is not None]\n",
    "    \n",
    "    return doc_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_normalized_corpus = df['Text'].apply(process_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [be, not, so, much, hard, people, find, challe...\n",
       "1    [so, unbearable, do, find, take, up, safe, ala...\n",
       "2    [men, press, button, deliver, painful, jolt, s...\n",
       "3    [same, condition, quarter, woman, press, shock...\n",
       "4    [report, psychologist, virginia, harvard, univ...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fully_normalized_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7599"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_fully_norm = pd.Series(list(itertools.chain(*fully_normalized_corpus)))\n",
    "len(flattened_fully_norm.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12108\n"
     ]
    }
   ],
   "source": [
    "## Original dictionary length\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "be             5304\n",
       "have           2035\n",
       "say            1224\n",
       "s               930\n",
       "people          763\n",
       "               ... \n",
       "rede              1\n",
       "metabolic         1\n",
       "luminescent       1\n",
       "teem              1\n",
       "exert             1\n",
       "Length: 7599, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_fully_norm.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #define attributes to store if text preprocessing requires fitting from data\n",
    "        pass\n",
    "    \n",
    "    def fit(self, data, y = 0):\n",
    "        # this is where you would fit things like corpus specific stopwords\n",
    "        # fit probable bigrams with bigram model in here\n",
    "        \n",
    "        # save as parameters of Text preprocessor\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, data, y = 0):\n",
    "        fully_normalized_corpus = data.apply(self.process_doc)\n",
    "        \n",
    "        return fully_normalized_corpus\n",
    "        \n",
    "    \n",
    "    def process_doc(self, doc):\n",
    "\n",
    "        #initialize lemmatizer\n",
    "        wnl = WordNetLemmatizer()\n",
    "        stop_words = stopwords.words('english')\n",
    "        \n",
    "        # helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "        def pos_tagger(nltk_tag):\n",
    "            if nltk_tag.startswith('J'):\n",
    "                return wordnet.ADJ\n",
    "            elif nltk_tag.startswith('V'):\n",
    "                return wordnet.VERB\n",
    "            elif nltk_tag.startswith('N'):\n",
    "                return wordnet.NOUN\n",
    "            elif nltk_tag.startswith('R'):\n",
    "                return wordnet.ADV\n",
    "            else:         \n",
    "                return None\n",
    "\n",
    "\n",
    "        # remove stop words and punctuations, then lower case\n",
    "        doc_norm = [tok.lower() for tok in word_tokenize(doc) if (tok.isalpha())]\n",
    "\n",
    "        #  POS detection on the result will be important in telling Wordnet's lemmatizer how to lemmatize\n",
    "\n",
    "        # creates list of tuples with tokens and POS tags in wordnet format\n",
    "        wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(doc_norm))) \n",
    "        doc_norm = [wnl.lemmatize(token, pos) for token, pos in wordnet_tagged if pos is not None]\n",
    "\n",
    "        return \" \".join(doc_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Text']\n",
    "y = df['Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "proc = TextPreprocessor()\n",
    "\n",
    "# again this kind of splitting only becomes important if fitting of text transformers fits to statistics of the text corpus\n",
    "transformed_train = proc.fit_transform(X_train)\n",
    "transformed_test = proc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_steps = [('countvec', CountVectorizer(min_df =.03, max_df = .5))]\n",
    "preprocess_pipeline = Pipeline(prc_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_proc = preprocess_pipeline.fit_transform(transformed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1984, 103)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_proc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_proc = preprocess_pipeline.transform(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accord</th>\n",
       "      <th>also</th>\n",
       "      <th>back</th>\n",
       "      <th>become</th>\n",
       "      <th>big</th>\n",
       "      <th>british</th>\n",
       "      <th>call</th>\n",
       "      <th>change</th>\n",
       "      <th>child</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>use</th>\n",
       "      <th>very</th>\n",
       "      <th>want</th>\n",
       "      <th>water</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>woman</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1984 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      accord  also  back  become  big  british  call  change  child  city  \\\n",
       "0          0     0     0       0    0        0     0       0      0     0   \n",
       "1          0     0     0       1    0        0     0       0      0     0   \n",
       "2          0     0     0       0    0        0     0       0      0     1   \n",
       "3          0     0     0       0    0        0     0       0      0     0   \n",
       "4          0     0     0       0    0        0     0       0      0     0   \n",
       "...      ...   ...   ...     ...  ...      ...   ...     ...    ...   ...   \n",
       "1979       0     0     0       0    0        0     0       0      0     0   \n",
       "1980       0     0     0       2    0        0     0       0      0     0   \n",
       "1981       0     0     0       0    0        0     0       1      1     0   \n",
       "1982       0     0     0       0    0        0     0       1      0     0   \n",
       "1983       0     1     0       0    0        0     0       0      0     0   \n",
       "\n",
       "      ...  use  very  want  water  way  well  woman  work  world  year  \n",
       "0     ...    1     0     0      0    0     0      0     0      0     1  \n",
       "1     ...    0     0     0      0    0     0      0     1      0     0  \n",
       "2     ...    1     0     1      0    0     0      0     0      0     2  \n",
       "3     ...    0     0     0      0    0     0      0     0      0     1  \n",
       "4     ...    0     0     0      0    0     0      0     0      0     0  \n",
       "...   ...  ...   ...   ...    ...  ...   ...    ...   ...    ...   ...  \n",
       "1979  ...    0     0     0      0    0     0      0     0      0     0  \n",
       "1980  ...    0     0     0      0    0     0      0     0      0     0  \n",
       "1981  ...    0     1     0      0    0     0      1     0      0     2  \n",
       "1982  ...    0     0     0      0    0     0      0     0      0     0  \n",
       "1983  ...    0     0     0      0    0     0      0     0      0     0  \n",
       "\n",
       "[1984 rows x 103 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_names = preprocess_pipeline[\n",
    "    'countvec'].get_feature_names()\n",
    "\n",
    "pd.DataFrame(X_tr_proc.toarray(), columns = feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accord</th>\n",
       "      <th>also</th>\n",
       "      <th>back</th>\n",
       "      <th>become</th>\n",
       "      <th>big</th>\n",
       "      <th>british</th>\n",
       "      <th>call</th>\n",
       "      <th>change</th>\n",
       "      <th>child</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>very</th>\n",
       "      <th>want</th>\n",
       "      <th>water</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>woman</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1984 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      accord  also  back  become  big  british  call  change  child  city  \\\n",
       "0          0     0     0       0    0        0     0       0      0     0   \n",
       "1          0     0     0       1    0        0     0       0      0     0   \n",
       "2          0     0     0       0    0        0     0       0      0     1   \n",
       "3          0     0     0       0    0        0     0       0      0     0   \n",
       "4          0     0     0       0    0        0     0       0      0     0   \n",
       "...      ...   ...   ...     ...  ...      ...   ...     ...    ...   ...   \n",
       "1979       0     0     0       0    0        0     0       0      0     0   \n",
       "1980       0     0     0       2    0        0     0       0      0     0   \n",
       "1981       0     0     0       0    0        0     0       1      1     0   \n",
       "1982       0     0     0       0    0        0     0       1      0     0   \n",
       "1983       0     1     0       0    0        0     0       0      0     0   \n",
       "\n",
       "      ...  very  want  water  way  well  woman  work  world  year  Level  \n",
       "0     ...     0     0      0    0     0      0     0      0     1    2.0  \n",
       "1     ...     0     0      0    0     0      0     1      0     0    NaN  \n",
       "2     ...     0     1      0    0     0      0     0      0     2    2.0  \n",
       "3     ...     0     0      0    0     0      0     0      0     1    NaN  \n",
       "4     ...     0     0      0    0     0      0     0      0     0    2.0  \n",
       "...   ...   ...   ...    ...  ...   ...    ...   ...    ...   ...    ...  \n",
       "1979  ...     0     0      0    0     0      0     0      0     0    NaN  \n",
       "1980  ...     0     0      0    0     0      0     0      0     0    NaN  \n",
       "1981  ...     1     0      0    0     0      1     0      0     2    0.0  \n",
       "1982  ...     0     0      0    0     0      0     0      0     0    0.0  \n",
       "1983  ...     0     0      0    0     0      0     0      0     0    0.0  \n",
       "\n",
       "[1984 rows x 104 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_mat = pd.DataFrame(X_tr_proc.toarray(), columns = feat_names)\n",
    "bow_mat['Level'] = y_train\n",
    "bow_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "have      0.085383\n",
       "say       0.063834\n",
       "people    0.035576\n",
       "not       0.026835\n",
       "more      0.023989\n",
       "do        0.021143\n",
       "world     0.019719\n",
       "make      0.019313\n",
       "year      0.016670\n",
       "want      0.015857\n",
       "dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2_bow_mat = bow_mat[bow_mat['Level'] == 2].drop(columns = ['Level'])\n",
    "\n",
    "# class 1 token probabilities:\n",
    "N_tok_2 = class2_bow_mat.sum(axis = 0) \n",
    "N_2 =  class2_bow_mat.values.sum()\n",
    "\n",
    "# get probabilities for each token: class 1\n",
    "proba_c2 = N_tok_2/N_2\n",
    "\n",
    "proba_c2.sort_values(ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "have      0.090177\n",
       "say       0.054490\n",
       "people    0.037222\n",
       "not       0.026669\n",
       "more      0.025902\n",
       "do        0.019186\n",
       "year      0.019186\n",
       "make      0.018227\n",
       "use       0.018035\n",
       "world     0.014582\n",
       "dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class1_bow_mat = bow_mat[bow_mat['Level'] == 1].drop(columns = ['Level'])\n",
    "\n",
    "# class 1 token probabilities:\n",
    "N_tok_1 = class1_bow_mat.sum(axis = 0) \n",
    "N_1 =  class1_bow_mat.values.sum()\n",
    "\n",
    "# get probabilities for each token: class 1\n",
    "proba_c1 = N_tok_1/N_1\n",
    "\n",
    "proba_c1.sort_values(ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "have      0.120313\n",
       "say       0.070312\n",
       "people    0.031250\n",
       "not       0.029687\n",
       "do        0.021875\n",
       "new       0.020313\n",
       "year      0.020313\n",
       "go        0.018750\n",
       "more      0.018750\n",
       "time      0.017188\n",
       "dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class0_bow_mat = bow_mat[bow_mat['Level'] == 0].drop(columns = ['Level'])\n",
    "\n",
    "# class 1 token probabilities:\n",
    "N_tok_0 = class0_bow_mat.sum(axis = 0)\n",
    "N_0 =  class0_bow_mat.values.sum() \n",
    "\n",
    "# get probabilities for each token: class 0\n",
    "proba_c0 = N_tok_0/N_0\n",
    "\n",
    "proba_c0.sort_values(ascending = False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Multinomial Naive Bayes Classifier to pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countvec', CountVectorizer(max_df=0.5, min_df=0.03)),\n",
       " ('multinb', MultinomialNB())]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "mod_pipe = deepcopy(preprocess_pipeline)\n",
    "mod_pipe.steps.append(('multinb', MultinomialNB()))\n",
    "mod_pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('countvec', CountVectorizer(max_df=0.5, min_df=0.03)),\n",
       "                ('multinb', MultinomialNB())])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_pipe.fit(transformed_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_test = proc.transform(X_test)\n",
    "\n",
    "y_pred = mod_pipe.predict(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.49      0.46       272\n",
      "           1       0.41      0.30      0.35       282\n",
      "           2       0.43      0.49      0.46       297\n",
      "\n",
      "    accuracy                           0.43       851\n",
      "   macro avg       0.43      0.43      0.42       851\n",
      "weighted avg       0.43      0.43      0.42       851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(cv=5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegressionCV(fit_intercept=True, cv= 5, \n",
    "                     dual=False, penalty='l2', scoring=None, \n",
    "                     solver='lbfgs', tol=0.0001, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'gamma': 1, 'kernel': 'linear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.46      0.45       272\n",
      "           1       0.53      0.23      0.32       282\n",
      "           2       0.42      0.62      0.50       297\n",
      "\n",
      "    accuracy                           0.44       851\n",
      "   macro avg       0.46      0.44      0.42       851\n",
      "weighted avg       0.46      0.44      0.42       851\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining parameter range \n",
    "param_grid = {'C': [0.1, 1, 10, 100],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              #'gamma':['scale', 'auto'],\n",
    "              'kernel': ['linear']}  \n",
    "   \n",
    "grid = GridSearchCV(SVC(random_state = 42), param_grid, refit = True, verbose = 3,n_jobs=-1) \n",
    "   \n",
    "# fitting the model for grid search \n",
    "grid.fit(X_tr_proc, y_train) \n",
    " \n",
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "grid_predictions = grid.predict(X_test_proc) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(y_test, grid_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFID Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_steps2 = [('tfid', TfidfVectorizer(min_df = 0.05, max_df = 0.95))]\n",
    "preprocess_pipeline2 = Pipeline(prc_steps2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_proc2 = preprocess_pipeline2.fit_transform(transformed_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_proc2 = preprocess_pipeline2.transform(transformed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'l' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-064389c32da4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# fitting the model for grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr_proc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# print best parameter after tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mkernel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_kernels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mlibsvm_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'l' is not in list"
     ]
    }
   ],
   "source": [
    "# defining parameter range \n",
    "param_grid = {'C': [0.1, 1, 10, 100],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              #'gamma':['scale', 'auto'],\n",
    "              'kernel': ['l']}  \n",
    "   \n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3,n_jobs=-1) \n",
    "   \n",
    "# fitting the model for grid search \n",
    "grid.fit(X_tr_proc2, y_train) \n",
    " \n",
    "# print best parameter after tuning \n",
    "print(grid.best_params_) \n",
    "grid_predictions2 = grid.predict(X_test_proc2) \n",
    "   \n",
    "# print classification report \n",
    "print(classification_report(y_test, grid_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.43      0.43       301\n",
      "           1       0.69      0.20      0.31       279\n",
      "           2       0.36      0.64      0.46       271\n",
      "\n",
      "    accuracy                           0.42       851\n",
      "   macro avg       0.50      0.42      0.40       851\n",
      "weighted avg       0.50      0.42      0.40       851\n",
      "\n",
      "[[129  12 160]\n",
      " [ 79  55 145]\n",
      " [ 85  13 173]]\n",
      "Accuracy of the model on Testing Sample Data: 0.4\n",
      "\n",
      "Accuracy values for 5-fold Cross Validation:\n",
      " [nan nan nan nan nan]\n",
      "\n",
      "Final Average Accuracy of the model: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 615, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 480, in _check_X_y\n",
      "    return self._validate_data(X, y, accept_sparse='csr')\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 795, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\", line 797, in __array__\n",
      "    return np.asarray(self.array, dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\", line 211, in __array__\n",
      "    return np.asarray(self._ndarray, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: 'In a concerted campaign – led, the panel found, by the Chief Constable, Peter Wright – the South Yorkshire Police put out their story that drunken supporters or those without tickets had caused the disaster. The victims had their blood tested for alcohol levels. This was “an exceptional decision ”, the panel said, for which it found “no rationale ”. When victims had alcohol in their blood, the police then checked to find if they had criminal records.'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 615, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 480, in _check_X_y\n",
      "    return self._validate_data(X, y, accept_sparse='csr')\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 795, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\", line 797, in __array__\n",
      "    return np.asarray(self.array, dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\", line 211, in __array__\n",
      "    return np.asarray(self._ndarray, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: '\\ufeffIt was not so much how hard people found the challenge but how far they would go to avoid it that left researchers gobsmacked. The task? To sit in a chair and do nothing but think. \\n'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 615, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 480, in _check_X_y\n",
      "    return self._validate_data(X, y, accept_sparse='csr')\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 795, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\", line 797, in __array__\n",
      "    return np.asarray(self.array, dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\", line 211, in __array__\n",
      "    return np.asarray(self._ndarray, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: '\\ufeffIt was not so much how hard people found the challenge but how far they would go to avoid it that left researchers gobsmacked. The task? To sit in a chair and do nothing but think. \\n'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 615, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 480, in _check_X_y\n",
      "    return self._validate_data(X, y, accept_sparse='csr')\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 795, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\", line 797, in __array__\n",
      "    return np.asarray(self.array, dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\", line 211, in __array__\n",
      "    return np.asarray(self._ndarray, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: '\\ufeffIt was not so much how hard people found the challenge but how far they would go to avoid it that left researchers gobsmacked. The task? To sit in a chair and do nothing but think. \\n'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 615, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 480, in _check_X_y\n",
      "    return self._validate_data(X, y, accept_sparse='csr')\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 795, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\", line 797, in __array__\n",
      "    return np.asarray(self.array, dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\", line 211, in __array__\n",
      "    return np.asarray(self._ndarray, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: '\\ufeffIt was not so much how hard people found the challenge but how far they would go to avoid it that left researchers gobsmacked. The task? To sit in a chair and do nothing but think. \\n'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    " \n",
    "# GaussianNB is used in Binomial Classification\n",
    "# MultinomialNB is used in multi-class classification\n",
    "#clf = GaussianNB()\n",
    "clf = MultinomialNB()\n",
    " \n",
    "# Printing all the parameters of Naive Bayes\n",
    "print(clf)\n",
    " \n",
    "NB=clf.fit(X_tr_proc,y_train)\n",
    "prediction=NB.predict(X_test_proc)\n",
    " \n",
    "# Measuring accuracy on Testing Data\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(metrics.confusion_matrix(y_test, prediction))\n",
    " \n",
    "# Printing the Overall Accuracy of the model\n",
    "F1_Score=metrics.f1_score(y_test, prediction, average='weighted')\n",
    "print('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n",
    " \n",
    "# Importing cross validation function from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "# Running 10-Fold Cross validation on a given algorithm\n",
    "# Passing full data X and y because the K-fold will split the data and automatically choose train/test\n",
    "Accuracy_Values=cross_val_score(NB, X , y, cv=5, scoring='f1_weighted')\n",
    "print('\\nAccuracy values for 5-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplementNB()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.59      0.48       301\n",
      "           1       1.00      0.16      0.28       279\n",
      "           2       0.37      0.48      0.42       271\n",
      "\n",
      "    accuracy                           0.42       851\n",
      "   macro avg       0.59      0.41      0.39       851\n",
      "weighted avg       0.59      0.42      0.39       851\n",
      "\n",
      "[[178   0 123]\n",
      " [129  46 104]\n",
      " [140   0 131]]\n",
      "Accuracy of the model on Testing Sample Data: 0.39\n",
      "\n",
      "Accuracy values for 5-fold Cross Validation:\n",
      " [nan nan nan nan nan]\n",
      "\n",
      "Final Average Accuracy of the model: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 615, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 480, in _check_X_y\n",
      "    return self._validate_data(X, y, accept_sparse='csr')\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 795, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\", line 797, in __array__\n",
      "    return np.asarray(self.array, dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\", line 211, in __array__\n",
      "    return np.asarray(self._ndarray, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: 'In a concerted campaign – led, the panel found, by the Chief Constable, Peter Wright – the South Yorkshire Police put out their story that drunken supporters or those without tickets had caused the disaster. The victims had their blood tested for alcohol levels. This was “an exceptional decision ”, the panel said, for which it found “no rationale ”. When victims had alcohol in their blood, the police then checked to find if they had criminal records.'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 615, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 480, in _check_X_y\n",
      "    return self._validate_data(X, y, accept_sparse='csr')\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 795, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\", line 797, in __array__\n",
      "    return np.asarray(self.array, dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\", line 211, in __array__\n",
      "    return np.asarray(self._ndarray, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: '\\ufeffIt was not so much how hard people found the challenge but how far they would go to avoid it that left researchers gobsmacked. The task? To sit in a chair and do nothing but think. \\n'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 615, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 480, in _check_X_y\n",
      "    return self._validate_data(X, y, accept_sparse='csr')\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 795, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\", line 797, in __array__\n",
      "    return np.asarray(self.array, dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\", line 211, in __array__\n",
      "    return np.asarray(self._ndarray, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: '\\ufeffIt was not so much how hard people found the challenge but how far they would go to avoid it that left researchers gobsmacked. The task? To sit in a chair and do nothing but think. \\n'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 615, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 480, in _check_X_y\n",
      "    return self._validate_data(X, y, accept_sparse='csr')\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 795, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\", line 797, in __array__\n",
      "    return np.asarray(self.array, dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\", line 211, in __array__\n",
      "    return np.asarray(self._ndarray, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: '\\ufeffIt was not so much how hard people found the challenge but how far they would go to avoid it that left researchers gobsmacked. The task? To sit in a chair and do nothing but think. \\n'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 615, in fit\n",
      "    X, y = self._check_X_y(X, y)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/naive_bayes.py\", line 480, in _check_X_y\n",
      "    return self._validate_data(X, y, accept_sparse='csr')\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/base.py\", line 432, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 795, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 72, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 598, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/series.py\", line 797, in __array__\n",
      "    return np.asarray(self.array, dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/pandas/core/arrays/numpy_.py\", line 211, in __array__\n",
      "    return np.asarray(self._ndarray, dtype=dtype)\n",
      "  File \"/Users/javm/opt/anaconda3/envs/learn-env/lib/python3.8/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: '\\ufeffIt was not so much how hard people found the challenge but how far they would go to avoid it that left researchers gobsmacked. The task? To sit in a chair and do nothing but think. \\n'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB\n",
    " \n",
    "# GaussianNB is used in Binomial Classification\n",
    "# MultinomialNB is used in multi-class classification\n",
    "#clf = GaussianNB()\n",
    "clf = ComplementNB()\n",
    " \n",
    "# Printing all the parameters of Naive Bayes\n",
    "print(clf)\n",
    " \n",
    "NB=clf.fit(X_tr_proc2,y_train)\n",
    "prediction=NB.predict(X_test_proc2)\n",
    " \n",
    "# Measuring accuracy on Testing Data\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, prediction))\n",
    "print(metrics.confusion_matrix(y_test, prediction))\n",
    " \n",
    "# Printing the Overall Accuracy of the model\n",
    "F1_Score=metrics.f1_score(y_test, prediction, average='weighted')\n",
    "print('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n",
    " \n",
    "# Importing cross validation function from sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "# Running 10-Fold Cross validation on a given algorithm\n",
    "# Passing full data X and y because the K-fold will split the data and automatically choose train/test\n",
    "Accuracy_Values=cross_val_score(NB, X , y, cv=5, scoring='f1_weighted')\n",
    "print('\\nAccuracy values for 5-fold Cross Validation:\\n',Accuracy_Values)\n",
    "print('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "b2421b8d9327a3c0803e8d31d88da22227b8da3ae4048c9f4a9f5f97cf759005"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
